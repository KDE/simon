<?xml version="1.0" ?>
<!DOCTYPE book PUBLIC "-//KDE//DTD DocBook XML V4.5-Based Variant V1.1//EN" "dtd/kdedbx45.dtd" [
  <!ENTITY kmyapplication "<application>Simon</application>">
  <!ENTITY kappname "&kmyapplication;">
  <!ENTITY package "kde-module">
  <!ENTITY % addindex "IGNORE">
  <!ENTITY % English "INCLUDE">

]>


<book id="simon" lang="&language;">

<bookinfo>
<title>The &kmyapplication; Handbook</title>

<copyright>
<year>2008-2012</year>
<holder>Peter Grasch</holder>
</copyright>

<date>2012-12-14</date>
<releaseinfo>0.4</releaseinfo>

<legalnotice>&FDLNotice;</legalnotice>

<authorgroup>
<author>
<personname>
<firstname>Peter</firstname>
<othername>H.</othername>
<surname>Grasch</surname>
</personname>
<email>peter.grasch@bedahr.org</email>
</author>
<!-- TRANS:ROLES_OF_TRANSLATORS -->
</authorgroup>

<abstract>
<para>
&kmyapplication; is an open source speech recognition solution.
</para>
</abstract>

<keywordset>
<keyword>KDE</keyword>
<keyword>kdeutils</keyword>
<keyword>Kapp</keyword>
<keyword>Simon</keyword>
<keyword>recognition</keyword>
<keyword>speech</keyword>
<keyword>voice</keyword>
<keyword>command</keyword>
<keyword>control</keyword>
<keyword>scenarios</keyword>
<keyword>acoustic</keyword>
<keyword>accessibility</keyword>
</keywordset>

</bookinfo>

<chapter id="introduction">
<title>Introduction</title>

<para>
&kmyapplication; is the main front end for the Simon open source speech recognition solution.

It is a Simond client and provides a graphical user interface for managing the speech model and the commands. Moreover, Simon can execute all sorts of commands based on the input it receives from the server: Simond.
</para>

<para>
In contrast to existing commercial offerings, Simon provides a unique do-it-yourself approach to speech recognition. Instead of predefined, pre-trained speech models, Simon does not ship with any model whatsoever. Instead, it provides an easy to use end-user interface to create language and acoustic models from scratch.
</para>

<para>
Additionally the end-user can easily download created use cases from other users and share his / her own.
</para>

<para>
The current release can be used to set up command-and-control solutions especially suitable for disabled people. However, because of the amount of training necessary, continuous, free dictation is neither supported nor reasonable with current versions of Simon.
</para>

<para>
Because of its architecture, the same version of Simon can be used with all languages and dialects. One can even mix languages within one model if necessary.
</para>
</chapter>


<chapter id="overview">
<title>Overview</title>

<sect1 id="architecture">
<title>Architecture</title>
<para>
The main recognition architecture of Simon consists of three applications.
<itemizedlist>
  <listitem><para>&kmyapplication;</para><para>This is the main graphical interface.</para><para>It acts as a client to the Simond server.</para></listitem>
  <listitem><para>Simond</para><para>The recognition server.</para></listitem>
  <listitem><para>KSimond</para><para>A graphical front-end for Simond.</para></listitem>
</itemizedlist>
</para>

<para>
These three components form a real client / server solution for the recognition. That means that there is one server (Simond) for one or more clients (&kmyapplication;; this application). KSimond is just a front-end for Simond which means it adds no functionality to the system but rather provides a way to interact with Simond graphically.
</para>

<para>
Additionally to the Simon, Simond and KSimond other, more specialized applications are also part of this integrated Simon distribution.
<itemizedlist>
  <listitem><para>Sam</para><para>Provides more in-depth control to your speech model and allows to test the acoustic model.</para></listitem>
  <listitem><para>SSC / SSCd</para><para>These two applications can be used to collect large amount of speech samples from different persons more easily.</para></listitem>
  <listitem><para>Afaras</para><para>This simple utility allows users to quickly check large corpora of speech data for erroneous samples.</para></listitem>
</itemizedlist>
</para>

<para>
Please refer to the individual handbooks of those applications for more details.
</para>


<para>
<screenshot>
<screeninfo>Architecture</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="architecture.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>&kmyapplication; is used to create and maintain a representation of your pronunciation and language. This representation is then sent to the server Simond which compiles it into a usable speech model.</para>
<para>&kmyapplication; then records sound from the microphone and transmits it to the server which runs the recognition on the received input stream. Simond sends the recognition result back to the client (&kmyapplication;).</para>
<para>&kmyapplication; then uses this recognition result to execute commands like opening programs, following links, &etc;</para>

<para>Simond identifies its connections with a user / password combination which is completely independent from the underlying operating system and its users. By default a standard user is set up in both Simon and Simond so the typical use case of one Simond server per Simon client will work <quote>out of the box</quote>.</para>
<para> Every Simon client logs onto the server with a user / password combination which identifies a unique user and thus a unique speech model. Every user maintains his own speech model but may use it from different computers (different, physical Simon instances) simply by accessing the same Simond server. One Simond instance can of course also serve multiple users.</para>
<para>If you want to open up the server to the Internet or use multiple users on one server, you will have to configure Simond. Please see the <ulink url="help:/simond">Simond manual</ulink> for details.
</para>
</sect1>

<sect1 id="needed_parts">
<title>Required Resources for a Working &kmyapplication; Setup</title>

<note>
<para>
For background information about speech models, please refer to the <link linkend="speech_model">Speech Recognition: Background section</link>.
</para>
</note>

<para>To get Simon to recognize speech and react to it you need to set up a speech model.</para>

<para>Speech models describe how your voice sounds, what words exist, how they sound and what word combination (<quote>sentences</quote> or <quote>structures</quote>) exist.</para>

<para>A speech model basically consists of two parts:
<itemizedlist>
<listitem><para>Language model: Describes all existing words and what sentences are grammatically correct </para></listitem>
<listitem><para>Acoustic model: Describes how words sound </para></listitem>
</itemizedlist>
</para>

<para>You need both these components to get Simon to recognize your voice.</para>

<para>In Simon, the language model will be created from your active <emphasis>scenarios</emphasis> and the acoustic model will be either built solely through your voice recordings (training) or with the help of a <emphasis>base model</emphasis>.</para>

<sect2 id="scenarios">
<title>Scenarios</title>

<para>
One scenario makes up one complete use case of Simon. To control Firefox, for example, the user just installs the Firefox scenario.
</para>

<para>
In other words, scenarios tell Simon what words and phrases to listen for and what to do when they are recognized.
</para>

<para>
Because scenarios do not contain information about how these words and phrases actually sound, they can be shared and exchanged between different Simon users without problems. To accommodate this community based repository pool, a <ulink url="http://kde-files.org/index.php?xcontentmode=692">category for Simon scenarios has been created on kde-files.org</ulink> where the scenarios, which are just simple text files (XML format), can be exchanged easily.
</para>

<para>
In most cases scenarios are tailored to work best with a specific <link linkend="base_model">base model</link> to avoid <link linkend="phoneme_set_issues">issues with the phoneme set</link>.
</para>

<para>
For information on how to use scenarios in Simon, please refer to the <link linkend="scenarios_use">Scenario section in the Use Simon chapter</link>.
</para>
</sect2>

<sect2 id="base_model">
<title>Acoustic model</title>

<para>As mentioned above, you need an acoustic model to activate Simon.</para>

<para>You can either create your own or use and even adapt a base model. Base models are already generated, most often speaker independent, acoustic models that can be used with Simon.</para>

<para>The following table shows what is required, depending on your Simon configuration:

<table frame='all'><title>Ways to an acoustic model</title>
<tgroup cols='4' align='left' colsep='1' rowsep='1'>
<thead>
<row>
  <entry></entry>
  <entry>Training required</entry>
  <entry>Base model required</entry>
  <entry><link linkend="model_backends">Model creation backend</link> required</entry>
</row>
</thead>
<tbody>
<row> <entry>Static base model</entry>
  <entry>No</entry>
  <entry>Yes</entry>
  <entry>No</entry>
</row>
<row>
  <entry>Adapted base model</entry>
  <entry>Yes</entry>
  <entry>Yes</entry>
  <entry>Yes</entry>
</row>
<row>
  <entry>User-generated model</entry>
  <entry>Yes</entry>
  <entry>No</entry>
  <entry>Yes</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<sect3 id="model_backends">
<title>Backends</title>

<para>
Simon uses external software to build acoustic models and to recognize speech.
</para>

<para>
Usually, these backends can be split into two distinct components: The "model compiler" or "model generation" backend used to create or adapt acoustic models and the "recognizer" used to recognize speech with the help of these models.
</para>

<para>
Not all operation modes of Simon will require a model compiler backend. Please refer to the next section about details on when this is the case.
</para>

<para>Two different backends are supported:

<itemizedlist>
<listitem><para>Julius / HTK</para>
<para>Models will be created with the <ulink url="http://htk.eng.cam.ac.uk">HTK</ulink>. <ulink url="http://julius.sourceforge.jp/en_index.php">Julius</ulink> will be used as recognizer.</para>
<para>To use this backend, please make sure that you have an up-to-date version of both these tools installed.</para>
</listitem>
<listitem><para>CMU SPHINX</para>
<para>This backend, also often simply referred to as "SPHINX backend", uses the PocketSphinx recognizer and the SphinxTrain model generation backend. Please refer to the <ulink url="http://cmusphinx.sourceforge.net/">CMU SPHINX website</ulink> for more details.</para>
<para>The CMU SPHINX backend requires that Simon is built with the optional SPHINX support. If you have not compiled Simon from source, please refer to your distribution for more information.</para>
</listitem>
</itemizedlist>
</para>

<para>If you are using base models, Simon will automatically select the appropriate backend for you. However, if you want to build your own models from scratch (user-generated model, see below) and have a certain preference, please <ulink url="help:/simond/speech_model_compilation_configuration.html">refer to the Simond configuration</ulink> for more information.</para>

<para>Base models created for one backend are not compatible with any other backend. Please refer to the <link linkend="base_model_compatibility">compatibility matrix</link> for details.</para>
</sect3>

<sect3 id="base_model_types">
<title>Types of base models</title>

<para>
There are three types of base models:

<itemizedlist>
<listitem><para>Static base model</para>
</listitem>
<listitem><para>Adapted base model</para>
</listitem>
<listitem><para>User-generated model</para>
</listitem>
</itemizedlist>
</para>

<para>
For information on how to use base models in Simon, please refer to the <link linkend="base_model_use">Base Models section in the Use Simon chapter</link>.
</para>

<sect4 id="base_model_static">
<title>Static base model</title>

<para>Static base models simply use a pre-compiled acoustic model without modifying it.</para>

<para>Any training data collected through Simon will not be used to improve the recognition accuracy.</para>

<para>This type of model <emphasis>does not</emphasis> require the model creation backend to be installed.</para>

</sect4>

<sect4 id="base_model_adapted">
<title>Adapted base model</title>

<para>By adapting a pre-compiled acoustic model you can improve accuracy by adapting it to your voice.</para>

<para>Collected training data will be compiled in an adaption matrix which will then be applied to the selected base model.</para>

<para>This type of model <emphasis>does</emphasis> require the model creation backend to be installed.</para>

</sect4>

<sect4 id="base_model_user_generated">
<title>User-generated model</title>

<para>When using user-generated models, the user is responsible for training his own model. No base model will be used.</para>

<para>The training data will be used to compile your own acoustic model allowing you to create a system which directly reflects your voice.</para>

<para>This type of model <emphasis>does</emphasis> require the model creation backend to be installed.</para>
</sect4>

</sect3>

<sect3 id="base_model_compatibility">
<title>Requirements</title>

<para>
To build, adapt or use acoustic models of different types, certain software needs to be installed.

<table frame='all'><title>Base model requirements</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry></entry>
  <entry>CMU SPHINX</entry>
  <entry>Julius / HTK</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Static base model</entry>
  <entry>PocketSphinx</entry>
  <entry>Julius</entry>
</row>
<row>
  <entry>Adapted base model</entry>
  <entry>SphinxTrain, PocketSphinx</entry>
  <entry>HTK, Julius</entry>
</row>
<row>
  <entry>User-generated model</entry>
  <entry>SphinxTrain, PocketSphinx</entry>
  <entry>HTK, Julius</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
All four tools, HTK, Julius, PocketSphinx and SphinxTrain, can safely be installed at the same time.
</para>
<para>
SPHINX support in Simon must be enabled during compile time and might not be available on your platform. Please refer to your distribution.
</para>
<note>
<para>
The Simon Windows installer includes Julius, PocketSphinx and SphinxTrain but not the HTK. Please refer to the <link linkend="installation">installation section</link> for information on how to install it should you find the need for it.
</para>
</note>
</sect3>

<sect3 id="base_model_get">
<title>Where to get base models</title>

<para>
Simon base models are packaged as <filename>.sbm</filename> files. If you happen to have raw model files for your backend, you can package them into a compatible SBM container within Simon. Please refer to the <link linkend="configuration_speechmodel">speech model configuration</link> for details.
</para>

<para>
Not all SBM models may work for you. Please refer to the <link linkend="model_backends">model backends section</link> for details.
</para>

<para>
To keep this list of available base models up to date, please refer to <ulink url="http://userbase.kde.org/Special:myLanguage/Simon/Base_models#Where_to_get_base_models">the list in our online wiki</ulink>.
</para>
</sect3>

<sect3 id="phoneme_set_issues">
<title>Phoneme set issues</title>

<para>In order for base models to work, both your scenarios and your base model need to use the same set of phonemes.</para>

<para>In practice, this often just means that you need to <emphasis>match scenarios to your base model</emphasis>. The name of &kmyapplication; base models will most likely start with a tag like "[EN/VF/JHTK]". Try to download scenarios that start with the same tag.</para>

<para>
You can not use scenarios designed for different phoneme set (different base model). If Simon recognizes this error, it will try to disable affected words by removing them from the created speech model. These words will be marked with a red background in the vocabulary of the scenario. To re-enable them, transcribe them with the proper phoneme set or use a user-generated model.
</para>

<note>
<title>Hint</title>
<para>If you design a new scenario it is therefore a good idea to use the dictionary that was used to create the base model as shadow dictionary. This way Simon will suggest the <quote>correct</quote> phonemes when adding the words automatically.</para>
</note>
</sect3>
</sect2>
</sect1>

</chapter>


<chapter id="using-simon">
<title>Using &kmyapplication;: Typical user</title>

<para>The following sections will describe how to use Simon.</para>

<sect1 id="first_run">
<title>First run wizard</title>

<para>On the first start of Simon, this assistant will guide you through the initial configuration of &kmyapplication;.</para>

<para>
  <screenshot>
    <screeninfo>First run: Welcome</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>The configuration consists of five easy steps which are outlined below. You can skip each step and even the whole wizard if you want to - in that case, the system will be set up with default values.</para>

<para>However, please note that without any configuration, there won't be any recognition.</para>

<sect2 id="first_run_scenarios">
<title>Scenarios</title>

<para>In this step you can add or download <link linkend="scenarios">scenarios</link>.</para>

<para>
  <screenshot>
    <screeninfo>First run: Scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>To download scenarios from the online repository, select <menuchoice><guimenuitem>Open</guimenuitem><guimenuitem>Download</guimenuitem></menuchoice> to open the download dialog pictured below.</para>

<para>
  <screenshot>
    <screeninfo>First run: Get scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="get_hot_new_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Especially for new users it is recommended to try some scenarios first to see how the system works before diving into configuring it exactly for your use case.</para>

<para>After completing the assistant, you can change the scenario configuration with the use of the <link linkend="scenarios_use">scenario management dialog</link>.</para>

<para>If you are planning to use a <link linkend="base_model">base model</link>, make sure that you download <link linkend="phoneme_set_issues">matching scenarios</link>.</para>

</sect2>


<sect2 id="first_run_basemodels">
<title>Base models</title>

<para>In this step you can set up Simon to use <link linkend="base_model">base models</link>.</para>

<para>
  <screenshot>
    <screeninfo>First run: Base models</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Again, you can download base models from an online repository through <menuchoice><guimenuitem>Open model</guimenuitem><guimenuitem>Download</guimenuitem></menuchoice>.</para>

<para>
  <screenshot>
    <screeninfo>First run: Base model download</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="base_model_settings_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>To use a user-generated model, select <guimenuitem>Do not use a base model</guimenuitem>.</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="base_model_use">Simon configuration</link>.</para>

</sect2>

<sect2 id="first_run_server">
<title>Server</title>

<para>Internally, Simon is a <link linkend="architecture">server / client application</link>. If you want to take advantage of a network based installation, you can provide the server address here.</para>

<para>
  <screenshot>
    <screeninfo>First run: Server</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>The default configuration is sufficient for a <quote>normal</quote> installation and will assume that you use a local Simond server that will be started automatically and stopped with Simon.</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="configure_server">server configuration</link>.</para>

</sect2>

<sect2 id="first_run_sound_configuration">
<title>Sound configuration</title>

<para>Because Simon recognizes sound from one or more microphones, you have to tell Simon which devices you want to use for recognition and training.</para>

<para>
  <screenshot>
    <screeninfo>First run: Sound configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_5.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Simon can use one or more input- and output devices for different tasks. You can find more information about Simon's multiple device capabilities in the <link linkend="soundconfiguration_device">Simon sound configuration section</link>.</para>

<para>If you don't have at least one working input device for recognition, you will not be able to activate Simon.</para>

<para>After completing or aborting the first run wizard you can change configuration options defined here in the <link linkend="soundconfiguration_device">sound configuration</link>.</para>

</sect2>

<sect2 id="first_run_volume_calibration">
<title>Volume calibration</title>

<para>For Simon to work correctly, you need to configure your microphones volume to a sensible level.</para>

<para>
  <screenshot>
    <screeninfo>First run: Volume calibration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="first_run_6.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>For more details on this, please see the general section about <link linkend="volume_calibration">Volume Calibration</link>.</para>

</sect2>

</sect1>

<sect1 id="main_window">
<title>The Simon Main Window</title>
<para>
<screenshot>
<screeninfo>A screenshot of &kmyapplication;</screeninfo>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="screenshot_main.png" format="PNG"/>
	  </imageobject>
	  <textobject>
	    <phrase>Screenshot</phrase>
	  </textobject>
	</mediaobject>
</screenshot>
</para>


<para>The Simon main window is split into four logical sections. On the top left, you can see the scenario section, to its right you find the training section, on the bottom left is the acoustic model and finally, on the right of that, the recognition section.</para>

<para>
The Simon main window can be hidden at any time by clicking on the Simon logo in the system tray (usually next to the system clock in the task bar) which will minimize Simon to the tray. Click it again to show the main window again.
</para>

<sect2 id="main_window_scenarios">
<title>Main window: Scenarios</title>

<para>A list of scenarios shows the currently loaded scenarios. You can manage this selection by clicking <guibutton>Manage scenarios</guibutton> which will open the <link linkend="scenarios_use">scenario management dialog</link>.</para>

<para>To <link linkend="using-simon-advanced">modify a scenario</link>, select it from the list and open it by pressing <guibutton>Open "&lt;name&gt;"</guibutton>.</para>
</sect2>

<sect2>
<title>Main window: Training</title>

<para>This section shows all training texts from all currently active scenarios.</para>

<para>Selecting a training text will highlight the parent scenario in the scenario section.</para>

<para>You can start to <link linkend="training_in_progress">train the recognition</link> by selecting a text and clicking on <guibutton>Start training</guibutton>. Please note that, depending on your selected model type, training may or may not improve your recognition accuracy. The acoustic model section (see below) in the &kmyapplication; main menu tells you if training will have an effect for your specific configuration. For more information, please refer to the <link linkend="base_model">base model section</link> for background information on this subject.</para>

<para>The gathered training corpus can be managed by selecting <guibutton>Manage training data</guibutton> which will open the <link linkend="sample_management">sample management dialog</link>.</para>

<para>To help build a general, open speech corpus, please consider contributing your training corpus to the <ulink url="http://voxforge.org">Voxforge</ulink> project by selecting <menuchoice><guimenuitem>File</guimenuitem><guimenuitem>Contribute samples</guimenuitem></menuchoice> to bring up the <link linkend="contribute_samples">sample upload assistant</link>.</para>
</sect2>

<sect2>
<title>Main window: Acoustic model</title>

<para>Here, &kmyapplication; shows information about the currently used base- and active model.</para>

<para>Select <guibutton>Configure acoustic model</guibutton> to <link linkend="base_model_use">configure the base model</link>.</para>
</sect2>

<sect2>
<title>Main window: Recognition</title>

<para>This section shows information about the recognition status.</para>

<para>If &kmyapplication; is connected to the server, you can activate and deactivate the recognition by toggling the <guibutton>Activate</guibutton> button. If this control element is not available, make sure you are connected by selecting <menuchoice><guimenu>File</guimenu><guimenuitem>Connect</guimenuitem></menuchoice> from &kmyapplication;s menu.</para>

<para>An integrated volume calibration widget monitors the configured recognition devices. The sound setup can be modified by selecting <guibutton>Configure audio</guibutton> to bring up the <link linkend="soundconfiguration">sound configuration</link>.</para>

</sect2>

</sect1>

<sect1 id="scenarios_use">
<title>Scenarios</title>
<para>This section describes how to import and remove scenarios to your Simon configuration. For general information about scenarios, please refer to <link linkend="scenarios">the background chapter</link>. If you want to create, edit or export scenarios, please refer to the <link linkend="scenarios_use_advanced">advanced usage section</link>.</para>

<para>To modify your scenario configuration, first open the scenario management dialog by pressing <guibutton>Manage scenarios</guibutton> in the Simon main window.</para>

<para>
  <screenshot>
    <screeninfo>Manage scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="manage_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>To activate or deactivate a scenario you can use the arrow buttons between the two lists or simply double click the option you want to load / unload.</para>

<para>More information about individual scenarios can be found in the tooltips of the list items.</para>

<sect2 id="import_scenario">
  <title>Import Scenario</title>
  <para>Scenarios can be imported from a local file in Simon's XML scenario file format but can also be directly downloaded and imported from the internet.</para>

  <para>When downloading scenarios, the list of scenarios is retrieved from <ulink url="https://store.kde.org/browse/cat/319/ord/latest/">Simon Scenarios</ulink> subsection of the OpenDesktop site <ulink url="https://store.kde.org/">KDE Store</ulink>.</para>

<para>
  <screenshot>
    <screeninfo>Download scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="get_hot_new_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

  <para>If you create a scenario that might be valuable for other Simon users, please consider <link linkend="export_scenario">uploading it</link> to this online repository and help other Simon users.</para>
</sect2>

<sect2 id="delete_scenario">
<title>Delete Scenario</title>

<para>To delete a scenario, select the scenario and click the <guibutton>Delete</guibutton> button.</para>

<para>Because scenarios are synchronized with the recognition server, you can restore deleted scenarios through the <link linkend="configure_synchronization">model synchronization backup</link>.</para>
</sect2>

</sect1>


<sect1 id="recording">
<title>Recordings</title>
<para>If you are using user-generated or adapted models, Simon builds its acoustic model based on transcribed samples of the users voice. Because of this, the recorded samples are of vital importance for the recognition performance.</para>

<sect2 id="volume">
<title>Volume</title>
<para>It is important that you check your microphone volume before recording any samples.</para>

<sect3 id="volume_calibration">
<title>Simon Calibration</title>

<para>The current version of &kmyapplication; includes a simple way of ensuring that your volume is configured correctly.</para>

<para>
<screenshot>
<screeninfo>Simon Volume Calibration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="volume_calibration.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>By default the volume calibration is displayed before starting any recording in &kmyapplication;.</para>

<para>To calibrate simply read the text displayed.</para>

<para>The calibration will monitor the current volume and tell you to either raise or lower the volume but you have to do that manually in your systems audio mixer.</para>

<para>During calibration, try to talk normally. Don't yell but don't be overly quiet either. Take into account that you should generally use the same volume setting for all your training and for the recognition too. You might speak a little bit louder (unconsciously) when you are upset or at another time of the day so try to raise your voice a little bit to anticipate this. It is much better to have a little quieter samples than to start clipping.</para>

<para>In the &kmyapplication; settings, both the text displayed and the levels considered correct can be changed. If you leave the text empty, the default text will be displayed. In the options you can also deactivate the calibration completely. See the <link linkend="soundconfiguration_training">training section</link> for more details.</para>
</sect3>

<sect3>
<title>Audacity Calibration</title>

<para>Alternatively you can use an audio editing tool like the free <ulink url="http://audacity.sourceforge.net">Audacity</ulink> to monitor the recording volume.</para>

<para>
Too quiet:
<screenshot>
<screeninfo>Volume: Too quiet</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="too_quiet.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Too loud:
<screenshot>
<screeninfo>Volume: Too loud</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="too_loud.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Perfect volume:
<screenshot>
<screeninfo>Perfect volume</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="volume_perfect.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

</sect2>

<sect2 id="silence">
<title>Silence</title>
<para>To help Simon with the automatic segmentation it is recommended to leave about one or two seconds of silence on the recording before and after reading the prompted text.</para>

<para>
Current Simon versions include a graphical notice on when to speak during recording. The message will tell the user to wait for about half a second:
<screenshot>
<screeninfo>Please wait</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="pause_control_wait.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
... before telling the user to speak:
<screenshot>
<screeninfo>Please speak</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="pause_control_speak.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
This method of visual feedback proved especially valuable when recording with people who cannot read the prompted text for themselves and therefore need someone to tell them what they have to say. The colorful visual cue tells them when to start repeating what the facilitator said without the need of unreliable hand gestures.
</para>
</sect2>


<sect2 id="recording_content">
<title>Content</title>
<para>Generally we recommend to record roughly the same sentences that Simon should recognize later.</para>
<para>(Obviously that does not apply to massive sample acquisitions where other properties like phonetic balance are more important)</para>

<para>Care should be taken to avoid recordings like <quote>One One One</quote> to quickly ramp up the <quote>recognition rate</quote> property. Such recordings often decrease recognition performance because the pronunciation differs greatly from saying the word in isolation.</para>
</sect2>


<sect2 id="microphone">
<title>Microphone</title>
<para>For Simon to work well, a high quality microphone is recommended.</para>

<para>However, even relatively cheap headsets (around 30 Euros) achieve very good results - magnitudes better than internal microphones.</para>

<para>For maximum compatibility we recommend USB headsets as they usually support the necessary samplerate of 16 kHz, are very well supported from both Microsoft Windows as well as GNU/Linux and normally don't require special, proprietary drivers to operate.</para>
</sect2>


<sect2 id="sample_quality_assurance">
<title>Sample Quality Assurance</title>
<para>Simon will check each recording against certain criteria to ensure that the recorded samples are not erroneous or of poor quality.</para>

<para>If Simon detects a problematic sample, it will warn the user to re-record the sample.</para>

<para>
Currently, Simon checks the following criteria:
<itemizedlist>
<listitem>
<para>Sample peak volume</para>
<para>If the volume is too loud and the microphone started to <quote>clip</quote> (<ulink url="http://en.wikipedia.org/wiki/Clipping_%28audio%29">Clipping on wikipedia</ulink>), Simon will display a warning message urging the user to lower the microphone volume.</para>
</listitem>
<listitem>
<para>Signal to noise ratio (SNR)</para>
<para>Simon will automatically determine the signal to noise ratio of each recording. If the ratio is below a configurable threshold, a warning message will be displayed.</para>
<para>The default value of 2300 % means that for Simon to accept a sample as correctly recorded the peak volume has to be 23 times louder than the noise baseline (lowest average over 50 ms).</para>
<para>
Often this can be a result of either a very low quality microphone, high levels of ambient noise or a low microphone gain coupled with a <quote>microphone boost</quote> option in the system mixer.
</para>
</listitem>
</itemizedlist>
</para>


<para>
SNR warning message triggered by an empty sample. This information dialog is displayed when clicking on the <guibutton>More information</guibutton> button on the recording widget.
<screenshot>
<screeninfo>Empty sample triggering the SNR warning</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="snr_warning.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>
</sect1>

<sect1 id="contribute_samples">
<title>Contribute Samples</title>

<para>The <link linkend="base_model">base models</link> that can be used with Simon to augment or replace training are built from other peoples speech samples. In order to create high quality base models, a large amount of training samples are necessary.</para>

<para>If you trained your local Simon installation, you gathered valuable voice samples that could improve the quality of the general model.</para>

<para>Through &kmyapplication;'s "Contribute Samples" dialog you can upload those recordings to benefit the <ulink url="http://voxforge.org">Voxforge</ulink> project to create high quality open source base models.</para>

<para>
<screenshot>
<screeninfo>Contributing Samples: Connect</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="contribute_samples_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>After connecting to the server, &kmyapplication; will ask for some basic meta-information. This information obviously contains no personal information. Instead, it will later be used to group together samples of similar speaker groups to build more accurate acoustic models.</para>

<para>
<screenshot>
<screeninfo>Contributing Samples: Provide information</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="contribute_samples_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The duration of the upload process itself will depend on your internet connection. Generally speaking, this only transmits relatively little data because the audio samples collected by &kmyapplication; are generally very small: around 0.1 MB per sample.</para>

<para>
<screenshot>
<screeninfo>Contributing Samples: Upload</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="contribute_samples_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect1>

<sect1 id="sample_management">
<title>Manage training data</title>

<para>To view and modify your personal training corpus, you can access the training data management dialog by selecting <guibutton>Manage training data</guibutton> in the &kmyapplication; main window or the <link linkend="training">training section of any opened scenario</link>.</para>

<para>
<screenshot>
<screeninfo>Manage training samples</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_samples.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<sect2 id="open_sample">
<title>Modifying samples</title>

<para>To listen to or re-record a sample, select it from the list and select <guibutton>Open Sample</guibutton>.</para>

<para>
<screenshot>
<screeninfo>Modifying a training sample</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_samples_modify.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>In this dialog you can also modify the sample's <link linkend="sample_group">group after it was recorded</link>.</para>

<para>If you remove the opened sample and do not re-record it, &kmyapplication; will offer to remove it from the corpus.</para>

<para>
<screenshot>
<screeninfo>Removing a training sample</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_samples_remove.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="clear_training_data">
<title>Clear training data</title>
<para>
After a confirmation dialog, this will remove all personal training data of the user.
</para>
</sect2>

<sect2 id="import_training-data">
<title>Importing Training Samples</title>
<para>
Using the import training data field you can import previously gathered training samples from previous Simon versions or manual training.</para>

<note>
<para>
This feature is very specific. Please use it with caution and make sure that you know exactly what you are doing before you continue.
</para>
</note>

<para>You can either provide a separate prompts file or let Simon extract the transcriptions from the filenames.</para>

<para>When using prompts based transcriptions your prompts file (UTF-8) needs to contain lines of the following content: <quote>[filename] [content]</quote>. Filenames are without file extensions and the content has to be uppercase. For example: <userinput>demo_2007_03_20 DEMO</userinput> to import the file <filename>demo_2007_03_20.wav</filename> containing the spoken word <quote>Demo</quote>.</para>

<para>Because prompts files do not contain a file extension, Simon will try wav, mp3, ogg and flac (in that order). If one of those match, no other extension will be tested and only the first file will be imported (in contrast to file based transcription where all files would be imported).</para>

<para>
When using file based transcriptions, a file called this_is_a_test.wav <emphasis>must</emphasis> contain <quote>This is a test</quote> and nothing else. Numbers and special characters (<quote>.</quote>, <quote>-</quote>,...) in the filename are ignored and stripped.
</para>

<para>Files recorded by Simon 0.2 <emphasis>will</emphasis> follow this naming scheme so you can safely import them using the file name extraction method. Files generated by previous Simon versions should not be imported using this function but you can use the prompts based import for that.</para>

<para>
Imported files and their transcription are then added to the training corpus.
</para>

<para>
To import a folder containing training samples just select the folder to import and depending on your import type also the prompts file.

<screenshot>
<screeninfo>Import training data wizard</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_training_data_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
The folder will be scanned recursively. This means that the given folder and all its subfolders will be searched for .wav, .flac, .mp3 and .ogg files. All files found will be imported.
</para>

<para>
When importing the sound files, all configured <link linkend="soundconfiguration_postprocessing">post processing filters</link> are applied.
</para>
<para>
  If you import anything other than WAV files you are responsible for decoding them during the import process (for example through post processing filters) or the model creation <emphasis>will</emphasis> fail.
</para>

</sect2>

</sect1>

<sect1 id="configuration">
<title>Configuration</title>
<para>
Simon was designed with high configurability in mind. Because of this, there are plentiful parameters that can be fine-tuned to your specific requirements.
</para>

<para>You can access Simon's configuration dialog through the application's main menu: <menuchoice><guimenuitem>Settings</guimenuitem><guimenuitem>Configure Simon...</guimenuitem></menuchoice>.</para>

<sect2 id="configuration_general">
<title>General Configuration</title>
<para>The general configuration page lists some basic settings.</para>

<para>If you want to show the <link linkend="first_run">first run assistant</link> again, deselect <guibutton>Disable configuration wizard</guibutton>.</para>

<para>
<screenshot>
<screeninfo>General Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_general.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>
Please note that the option to start Simon at login will work on both Microsoft Windows and when you are using KDE's Plasma on Linux. Support for other desktop environments like Gnome, XFCE, &etc; might require manually placing Simon in the session autostart (please refer to the respective manuals of your desktop environment).
</para>
<para>
When the option to start Simon minimized is selected, Simon will minimize to the system tray immediately after starting.
</para>
<para>
  Deselecting the option to warn when there are problems with samples deactivates the <link linkend="sample_quality_assurance">sample quality assurance</link>.
</para>
</sect2>

<sect2 id="soundconfiguration">
<title>Recordings</title>
<para>
Simon uses fairly sophisticated internal sound processing to enable complex multi-device setups.
</para>

<sect3 id="soundconfiguration_device">
<title>Device Configuration</title>
<para>The sound device configuration allows you to choose which sound device(s) to use, configure them and define additional recording parameters.</para>

<para>Use the <guibutton>Refresh devices</guibutton> button if you have plugged in additional sound devices since you started Simon.</para>

<para>
<screenshot>
<screeninfo>Sound device configuration: General</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Most of the time you will want to use 1 channel and 16kHz (which is also the default) because the recognition only works on mono input and works best at 16kHz (8kHz and 22kHz being other viable options). Some low-cost sound cards might not support this particular mode in which case you can enable automatic resampling in the device's advanced configuration.</para>

<note><para>Only change the channel and the samplerate if you really know what you are doing. Otherwise the recognition will <emphasis>most likely</emphasis> not work.</para></note>

<para>
<screenshot>
<screeninfo>Sound device configuration: Advanced options</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_advanced.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>You can use Simon with more than one sound device at the same time. Use <guibutton>Add device</guibutton> to add a new device to the configuration and <guibutton>Remove device</guibutton> to remove it from your configuration. The first device in your sound setup cannot be removed.</para>

<para>For each device you can determine for what you want the device to be used: Training or recognition (last one only applicable for input devices).</para>

<para>If you use more than one device for training, you will create multiple sound files for each utterance. When using multiple devices for recognition each one feeds a separate sound input stream to the server resulting in recognition results for each stream.</para>

<para>If you use multiple output devices the playback of the training samples will play on all configured audio devices.</para>

<para>When using different sample rates for your input devices, the output will only play on matching output devices. If you for example have one input device configured to use 16kHz and the other to use 48kHz, the playback of samples generated by the first one will only play on 16kHz outputs, the other one only on 48kHz devices.</para>

<para>In the device's advanced configuration, you can also define the <link linkend="sample_group">sample group tag</link> of the produced training samples and set activation <link linkend="context_conditions">context conditions</link>.</para>

<para>If you set up this device to be used for recognition and (any of) it's activation requirements are not met, the device will not record. This can be used to augment or even replace the <link linkend="soundconfiguration_vad">traditional voice activity detection</link> with context information.</para>

<para>For example, add a <link linkend="face_detection_condition">face detection condition</link> to the recording devices activation requirements to only enable the recognition when you're looking at the webcam.</para>

</sect3>

<sect3 id="soundconfiguration_vad">
  <title>Voice Activity Detection</title>

  <para>The recognition is done on the Simond server. See the <link linkend="architecture">architecture section</link> for more details.</para>

  <para>The sound stream is not continuous but is segmented by the Simon client. This is done by something called <quote>voice activity detection</quote>.</para>

  <para>
  <screenshot>
  <screeninfo>Voice activity detection</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="vad.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Here you can configure this segmentation through the following parameters:
    <itemizedlist>
      <listitem>
        <para><guilabel>Cutoff level</guilabel></para>
        <para>Everything below this level is considered <quote>silence</quote> (background noise).</para>
      </listitem>
      <listitem>
        <para><guilabel>Head margin</guilabel></para>
        <para>Cache for as long as head margin to start consider it a real sample. During this whole time the input level needs to be above the cutoff level.</para>
      </listitem>
      <listitem>
        <para><guilabel>Tail margin</guilabel></para>
        <para>After the recording went below the cutoff level, Simon will wait for as long as tail margin to consider the current recording a finished sample.</para>
      </listitem>
      <listitem>
        <para><guilabel>Skip samples shorter than</guilabel></para>
        <para>Samples that are shorter than this value are not considered for recognition (coughs, &etc;).</para>
      </listitem>
    </itemizedlist>
  </para>
</sect3>

<sect3 id="soundconfiguration_training">
  <title>Training settings</title>

  <para>
  <screenshot>
  <screeninfo>Training settings</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="training_config.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>When the option <guibutton>Default to power training</guibutton> is selected, Simon will, when training, automatically start- and stop the recording when displaying and hiding (respectively) the recording prompt. This option only sets the default value of the option, the user can change it at any time before beginning a training session.</para>

<para>The configurable font here refers to the text that is recorded to train the acoustic model (through explicit training or when adding a word).</para>

<para>This option has been introduced after we have worked with a few clients suffering spastic disability. While we used the mouse to control Simon during the training, they had to read what was on the screen. At first this was very problematic as the regular font size is relatively small and they had trouble making out what to read. This is why we made the font and the font size of the recording prompt configurable.</para>

<para>Here you can also define the required signal to noise ratio for Simon to consider a training sample to be correct. See the <link linkend="sample_quality_assurance">Sample Quality Assurance</link> section for more details.</para>
<para>On this configuration page you can also set the parameters for the <link linkend="volume_calibration">volume calibration</link>.</para>

<para>It can be deactivated for both the add word dialog and the training wizard by unchecking the group box itself.</para>

<para>The calibration itself uses the <link linkend="soundconfiguration_vad">voice activity recognition</link> to score your sound configuration.</para>

<para>The prompted text can be configured by entering text in the input field below. If the edit is empty a default text will be used.</para>
</sect3>

<sect3 id="soundconfiguration_postprocessing">
<title>Postprocessing</title>
<para>All recorded (training) and imported (through the import training data) samples can be processed using a series of postprocessing commands. Postprocessing chains are an advanced feature and shouldn't be needed by the average user.</para>

<para>
<screenshot>
<screeninfo>Sound Configuration: Postprocessing</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The postprocessing commands can be seen as a chain of filters through which the recordings have to pass through. Using these <quote>filters</quote> one could define commands to suppress background noise in the training data or normalize the recordings.</para>

<para>Given the program <application>process_audio</application> which takes the input- and output files as its arguments (&eg;: <command>process_audio in.wav out.wav</command>) the postprocessing command would be: <userinput>process_audio %1 %2</userinput>. The two placeholders %1 and %2 will be replaced by the input filename and the output filename respectively.</para>

<para>The switch to <quote>apply filters to recordings recorded with Simon</quote> enables the postprocessing chains for samples recorded during the training (including the initial training while adding the word). If you don't select this switch the postprocessing commands are only applied to imported samples (through the <link linkend="import_training-data">import training data wizard</link>).</para>

</sect3>

<sect3 id="configuration_sound_context">
<title>Context</title>

<para>Every sample recorded with Simon is assigned a <link linkend="sample_group">sample group</link>.</para>

<para>When creating the acoustic model from the training samples Simon can take the current situation into account to only use a subset of all gathered training data.</para>

<para>
<screenshot>
<screeninfo>Sound Configuration: Context</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_sound_5.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>For example, in a system where multiple, very different speakers use one shared setup, <link linkend="context_conditions">context conditions</link> can be set up to automatically build separate models for both users depending on the current situation.</para>

<para>The above screenshot, for example, shows a setup where, given that all samples of "peter" were tagged "peters_samples" and all samples of "mathias" were tagged "mathias_samples" (refer to the <link linkend="soundconfiguration_device">device configuration</link> for more information on how to set up sample groups), the active acoustic model will only contain the current user's own samples as long as the file <filename>/home/bedahr/.username</filename> contains either "peter" or "mathias".</para>

<para>Another example use-case would be to switch to a more noise-resistant acoustic model when the user starts playing music.</para>

</sect3>
</sect2>

<sect2 id="configuration_speechmodel">
<title>Speech Model</title>
<para>Here you can adjust the parameters of the speech model.</para>

<sect3 id="base_model_use">
<title>Base model</title>

<para>You can optionally use base models to limit / circumvent the training or to avoid installing a model creation backend. Please refer to the general <link linkend="base_model">base model section</link> for more details about base models.</para>

<para>
<screenshot>
<screeninfo>Base model configuration</screeninfo>
<mediaobject>
<imageobject>
<imagedata fileref="base_model_settings_1.png" format="PNG"/>
</imageobject>
</mediaobject>
</screenshot>
</para>

<para>To use a <link linkend="base_model_user_generated">user-generated model</link>, select <guimenuitem>Do not use a base model</guimenuitem>. To use a <link linkend="base_model_static">static base model</link>, select a base model and <emphasis>do not</emphasis> select <guibutton>Adapt base model using training samples</guibutton>. To instead use an <link linkend="base_model_adapted">adapted base model</link>, check <guibutton>Adapt base model using training samples</guibutton> after selecting a base model.</para>

<para>Simon base models are packaged in <filename>.sbm</filename> files.</para>

<para>To add base models to the selection, you can either import local models (<menuchoice><guimenuitem>Open model</guimenuitem><guimenuitem>Import</guimenuitem></menuchoice>), download them from an online repository (<menuchoice><guimenuitem>Open model</guimenuitem><guimenuitem>Download</guimenuitem></menuchoice>) or create new ones from raw files (<menuchoice><guimenuitem>Open model</guimenuitem><guimenuitem>Create from model files</guimenuitem></menuchoice>).</para>

<para>
  <screenshot>
    <screeninfo>Importing a base model from the internet</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="base_model_settings_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>If you have raw model files produced by either supported <link linkend="model_backends">model creation backend</link>, you can package them into SBM container for use with &kmyapplication;.</para>

<para>
  <screenshot>
    <screeninfo>Creating base models from raw files</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="base_model_settings_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>You can also export your currently active model by selecting <guibutton>Export active model</guibutton>. The exported SBM file will contain your full acoustic model (ignoring the <link linkend="configuration_sound_context">current context</link>) that can be <ulink url="https://store.kde.org/browse/cat/318/ord/latest/">shared with other Simon users</ulink>.</para>
</sect3>

<sect3>
<title>Training data</title>

<para>This section allows to configure the training samples.</para>

<para>
<screenshot>
<screeninfo>Training data configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_speech_model.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>The samplerate set here is the target samplerate of the acoustic model. It has nothing to do with the recording samplerate and it is the responsibility of the user to ensure that the samples are actually made available in that format (usually by recording in that exact samplerate or by defining postprocessing commands that resample the files; see the <link linkend="soundconfiguration">sound configuration section</link> for more details).</para>

<para>Usually either 16kHz or 8kHz models are built / used. 16kHz models will have higher accuracy over 8kHz models. Going higher than 16kHz is not recommended as it is very cpu-intensive and in practice probably won't result in higher recognition rates.</para>

<para>Moreover, the path to the training samples can be adjusted. However, be sure that the previously gathered training samples are also moved to the new location. If you use automatic synchronization the Simond would alternatively also provide Simon with the missing sample but copying them manually is still recommended for performance reasons.</para>
</sect3>


<sect3 id="configure_language_profile">
<title>Language Profile</title>

<para>In the language profile section you can select a previously built or downloaded <link linkend="language_profile_general">language profile</link> to aid with the transcription of new words.</para>

<para>
<screenshot>
<screeninfo>Language profile configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="language_profile_configuration.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>
</sect2>

<sect2 id="configuration_model_extensions">
<title>Model Extensions</title>

<para>Here you can configure the base URL that is going to be used for the <link linkend="import_dict_hadifix">automatic bomp import</link>. The default points to the copy on the Simon listens server.</para>
<para>
<screenshot>
<screeninfo>Model Extensions</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_model_extensions.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2 id="configure_recognition">
<title>Recognition</title>

<para>Here you can configure the recognition and model synchronization with the Simond server.</para>

<sect3 id="configure_server">
  <title>Server</title>

  <para>Using the server configuration you can set parameters of the connection to Simond.</para>

<sect4 id="configure_server_general">
<title>General</title>
<para>
The Simon main application connects to the Simond server (see the <link linkend="architecture">architecture section</link> for more information).
</para>

<para>
<screenshot>
<screeninfo>Configure Server: General</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_simond_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
To identify individual users of the system (one Simond server can of course serve multiple Simon clients), Simon and Simond use users. Every user has his own speech model. The username / password combination given here is used to log in to Simond. If Simond does not know the username or the password is incorrect, the connection will fail. See the <ulink url="help:/simond">Simond manual</ulink> on how to setup users for Simond.</para>

<para>The recognition itself - which is done by the server - might not be available at all times. For example it would not be possible to start the recognition as long as the user does not have a compiled acoustic and language model which has to be created first (during synchronization when all the ingredients - vocabulary, grammar, training - are present). Using the option to start the recognition automatically once it is available, Simon will request to start the recognition when it receives the information that it is ready (all <link linkend="needed_parts">required components</link> are available).</para>

<para>Using the <guilabel>Connect to server on startup</guilabel> option, Simon will automatically start the connection to the configured Simond servers after it has finished loading the user interface.</para>
</sect4>

<sect4 id="configure_server_network">
<title>Network</title>
<para>Simon connects to Simond using TCP/IP.</para>

<para>
<screenshot>
<screeninfo>Configure Server: Network</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_simond_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>
As of now (Simon 0.4), encryption is not yet supported.
</para>
<para>
The timeout setting specifies, how long Simon will wait for a first reply when contacting the hosts. If you are on a very, very slow network and/or use <quote>connect on start</quote> on a very slow machine, you may want to increase this value if you keep getting timeout errors and can resolve them by trying again repeatedly.
</para>
<para>Simon supports to be configured to use more than one Simond. This is very useful if you for example are going to use Simon on a laptop which connects to a different server depending where you are. You could for example add the server you use when you are home and the server used when you are at work. When connecting, Simon will try to connect to each of the servers (in order) until it finds one server that accepts the connection.</para>

<para>To add a server, just enter the host name or IP address and the port (separated by <quote>:</quote>) or use the dialog that appears when you select the blue arrow next to the input field.</para>

</sect4>
</sect3>

<sect3 id="configure_synchronization">
<title>Synchronization and Model Backup</title>
<para>Here you can configure the model synchronization and restore older versions of your speech model.</para>

<para>
<screenshot>
<screeninfo>Synchronization and Model Backup</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_synchronization.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Simon creates the speech input files which are then compiled and used by the Simond server (see the <link linkend="architecture">section architecture</link> for more details).
</para>

<para>
The process of sending the speech input files, compiling them and receiving the compiled versions is called <quote>synchronization</quote>.
Only after the speech model is synchronized the changes take effect and a new restore point is set. This is why per default Simon will always synchronize the model with the server when it changes. This is called <guimenuitem>Automatic Synchronization</guimenuitem> and is the recommended setting.
</para>
<para>
However, if you want more control you can instruct Simon to ask you before starting the synchronization after the model has changed or to rely on manual synchronization all together. When selecting the manual synchronization you have to manually use the <menuchoice><guimenuitem>Actions</guimenuitem><guimenuitem>Synchronize</guimenuitem></menuchoice> menu item of the Simon main window every time you want to compile the speech model.
</para>

<para>The Simon server will maintain a copy of the last five iterations of model files. However, this only includes the <quote>source files</quote> (the vocabulary, grammar, &etc;) - <emphasis>not the compiled model</emphasis>. However, the compiled model will be regenerated from the restored source files automatically.</para>

<para>After you have connected to the server, you can select one of the available models and restore it by clicking on <guimenuitem>Choose  Model</guimenuitem>.</para>

</sect3>
</sect2>



<sect2 id="configure_actions">
<title>Actions</title>

<para>In the actions configuration you can configure the reactions to recognition results.</para>

<sect3 id="configure_actions_recognition">
  <title>Recognition</title>

  <para>The recognition of Simon computes not only the most likely result but rather the top ten results.</para>
  <para>Each of the results are assigned a confidence score between 0 and 1 (where 1 is 100% sure).</para>
  <para>Using the <guilabel>Minimum confidence</guilabel> you can set a minimum confidence for recognition results to be considered valid.</para>

  <para>If more than one recognition results are rated higher than the minimum confidence score, Simon will provide a popup listing the most likely options for you to choose from.</para>

  <para>
  <screenshot>
  <screeninfo>Did you mean...?</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="did_you_mean.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>This popup can be disabled using the <guibutton>Display selection popup for ambiguous results</guibutton> check box.</para>
</sect3>
<sect3>
  <title>Dialog font</title>
  <para>Many plugins of Simon have a graphical user interface.</para>
  <para>The fonts of these interfaces can be configured centrally and independent of the systems font settings here.</para>

  <para>
  <screenshot>
    <screeninfo>Input number with custom font</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="input_number_huge_font.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect3>

<sect3>
  <title>Lists</title>
  <para>Here you can find the global list element configuration. This serves as a template for new scenarios but is also directly used for <link linkend="configure_actions_recognition">the popup for ambiguous recognition results</link>.</para>
</sect3>
</sect2>

<sect2 id="text-to-speech_config">
<title>Text-to-speech</title>

<para>Some parts of &kmyapplication;, most notably the <link linkend="dialog_command_plugin">dialog command plugin</link> employ text-to-speech (or "TTS") to read text aloud.</para>

<sect3>
<title>Backends</title>
<para>Multiple external TTS solutions can be used to allow &kmyapplication; to talk. Multiple backends can be enabled at the same time and will be queried in the configured order until one is found that can synthesize the requested message.</para>

<para>The following backends are available:
<itemizedlist>
  <listitem><para>Recordings</para><para>Instead of an engine to convert arbitrary text into speech, text-snippets can be pre-recorded and will be simply played back.</para></listitem>
  <listitem><para>Jovie</para><para>Uses the <ulink url="http://www.kde.org/applications/utilities/jovie">Jovie TTS system</ulink>. This requires a valid Jovie set-up.</para></listitem>
  <listitem><para>Webservice</para><para>The webservice backend can be used to talk to any TTS engine that has a web front-end that returns <filename>.wav</filename> files.</para></listitem>
</itemizedlist>
</para>
<para>
  <screenshot>
    <screeninfo>TTS: backend selection</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="tts_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>
</sect3>

<sect3>
<title>Recordings</title>

<para>Instead of using an external TTS engine, you can also record yourself or other speakers reading the texts aloud. Simon can then play back these pre-recorded snippets when they are requested of its text-to-speech engine.</para>

<para>These recorded sound bites are organized into "sets" of different speakers which can also be imported and exported to share them with other &kmyapplication; users.</para>

<para>
  <screenshot>
    <screeninfo>TTS: recordings</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="tts_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>
</sect3>

<sect3>
<title>Webservice</title>
<para>Through the webservice backend, &kmyapplication; can use web-based TTS engines like <ulink url="http://mary.dfki.de">MARY</ulink>.</para>

<para>
  <screenshot>
    <screeninfo>TTS: webservice</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="tts_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>
<para>You can provide any URL. &kmyapplication; will replace any instance of "%1" within the configured URL with the text to synthesize. The backend expects the queried webservice to return a <filename>.wav</filename> file that will be streamed and outputted through &kmyapplication;'s sound layer - respecting the <link linkend="soundconfiguration_device">sound device configuration</link>.</para>
</sect3>
</sect2>

<sect2 id="social_desktop_config">
<title>Social desktop</title>
<para>Scenarios can be <link linkend="export_scenario">uploaded</link> and <link linkend="import_scenario">downloaded from within Simon</link>.</para>

<para>For this we use KDEs social desktop facilities and our own <ulink url="https://store.kde.org/browse/cat/319/ord/latest/">category for Simon scenarios on KDE Store</ulink>.</para>

<para>If you already have an account on <ulink url="https://opendesktop.org">opendesktop.org</ulink> you can input the credentials there. If you don't, you can register directly in the configuration module.</para>

<para>The registration is of course free of charge.</para>
</sect2>

<sect2 id="webcam_configuration">
<title>Webcam configuration</title>
<para>In Webcam configuration, you can configure frames per second (fps) and select the webcam to use when multiple webcams are connected to your system. </para>
<para>
  <screenshot>
    <screeninfo>Webcam configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="configure_webcam.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para> Frames per second is the rate at which the webcam will produce unique consecutive images called frames. The optimal value of fps is between 5-15 for proper performance. </para>
</sect2>

<sect2 id="jconfs">
<title>Advanced: Adjusting the recognition parameters manually</title>

<para>
Simon is targeted towards end-users. Its interface is designed to allow even users without any background in speech technology to design their own language and acoustic models by providing reasonable default values for simple uses.
</para>
<para>
In special cases (severe speech impairments for example), special configuration might be needed. This is why the raw configuration files for the recognition are also respected by Simon and can of course be modified to suit your needs.
</para>

<sect3 id="julius">
<title>Julius</title>

<para>There are basically two parts of the Julius configuration that can be adjusted:
<itemizedlist>
<listitem><para>adin.jconf</para><para>This is the configuration of the Simon client of the sound stream sent from Simon to the Simond. This file is directly read by the adinstreamer.</para>
<para>Simon ships with a default adin.jconf without any special parameters. You can change this system wide configuration which will affect all users if there are different user accounts on your machine who all use Simon. To just change the configuration of one of those users copy the file to the user path (see below) and edit this copy.</para>
</listitem>
<listitem><para>julius.jconf</para><para>This is a configuration of the Simond server and directly influences the recognition. This file is parsed by libjulius and libsent directly.</para>
<para>Simond ships with a default julius.jconf. Whenever there is a new user added to the Simond database, Simond will automatically copy this system wide configuration to the new user. After that the user is of course free to change it but it won't affect the other users. This way the <quote>template</quote> (the system wide configuration) can be changed without affecting other users.</para>
</listitem>
</itemizedlist>
</para>

<para>The path to the Julius configuration files will depend on your platform:

<table frame='all'><title>Julius Configuration Files</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>File</entry>
  <entry>Microsoft Windows</entry>
  <entry>GNU/Linux</entry>
</row>
</thead>
<tbody>
<row>
  <entry>adin.jconf (system)</entry>
  <entry>(installation path)\share\apps\simon\adin.jconf</entry>
  <entry>`kde4-config --prefix`/share/apps/simon/adin.jconf</entry>
</row>
<row>
  <entry>adin.jconf (user)</entry>
  <entry>%appdata%\.kde\share\apps\simon\adin.jconf</entry>
  <entry>~/.kde/share/apps/simon/adin.jconf</entry>
</row>
<row>
  <entry>julius.jconf (template)</entry>
  <entry>(installation path)\share\apps\simond\default.jconf</entry>
  <entry>`kde4-config --prefix`/share/apps/simond/default.jconf</entry>
</row>
<row>
  <entry>julius.jconf (user)</entry>
  <entry>%appdata%\.kde\share\apps\simond\models\(user)\active\julius.jconf</entry>
  <entry>~/.kde/share/apps/simond/models/(user)/active/julius.jconf</entry>
</row>
</tbody>
</tgroup>
</table>
</para>
</sect3>

</sect2>

</sect1>

</chapter>


<chapter id="using-simon-advanced">
<title>Advanced: Creating new scenarios with &kmyapplication;</title>

<para>The following chapter is aimed towards more experienced users who want to design their own scenarios.</para>

<para>For general usage instruction, please refer to the chapter <link linkend="using-simon">Using &kmyapplication;: Typical user</link>.</para>

<sect1 id="creating_scenarios_general">
<title>Introduction</title>
<para>To add a new scenario, you first create a new scenario "shell" by <link linkend="add_scenario">adding a new scenario object</link> and then <link linkend="main_window_scenarios">open it in the &kmyapplication; main window</link>.</para>

<para>To instead modify an existing scenario, you of course just have to open it.</para>

<para>
A &kmyapplication; scenario contains the following components:
<itemizedlist>
<listitem><para><link linkend="vocabulary">Vocabulary</link></para></listitem>
<listitem><para><link linkend="grammar">Grammar</link></para></listitem>
<listitem><para><link linkend="training">Training texts</link></para></listitem>
<listitem><para><link linkend="context">Context</link></para></listitem>
<listitem><para><link linkend="commands">Commands</link></para></listitem>
</itemizedlist>
</para>

<para>Before describing how to configure these elements in Simon, the next section provides background information that will help you understand the basic principles of speech modelling. This fundamental knowledge is necessary to design sensible scenarios.</para>

</sect1>

<sect1 id="speech_model">
<title>Speech recognition: background</title>

<note><para>Before explaining exactly how you can create new scenarios with Simon, this section introduces some fundamental basics to speech recognition in general.</para></note>

<para>
Speech recognition systems take voice input (often from a microphone) and try to translate it into written text. To do that, they rely on statistical representations of human voice. To put it into simple terms: The computer learns how words - or more correctly the sounds that make up those words - sound.
</para>

<para>
A speech model consists of two distinct parts:
<itemizedlist>
  <listitem><para>Language Model</para></listitem>
  <listitem><para>Acoustic Model</para></listitem>
</itemizedlist>
</para>

<sect2 id="language_model">
<title>Language Model</title>

<para>The language model defines the vocabulary and the grammar you want to use.</para>

<sect3 id="vocabulary_theory">
<title>Vocabulary</title>
<para>
The vocabulary defines what words the recognition process should recognize. Every word you want to be able to use with Simon should be contained in your vocabulary.
</para>
<para>
One entry in the vocabulary defines exactly one <quote>word</quote>. In contrast to the common use of the word <quote>word</quote>, in Simon <quote>word</quote> means one unique combination of the following:
<itemizedlist>
  <listitem><para>Wordname</para><para>(The written word itself)</para></listitem>
  <listitem><para>Category</para><para>(Grammatical category; for example: <quote>Noun</quote>, <quote>Verb</quote>, &etc;)</para></listitem>
  <listitem><para>Pronunciation</para><para>(How the word is pronounced; Simon accepts any kind of phonetic as long as it does not use special characters or numbers)</para></listitem>
</itemizedlist>

That means that plurals or even different cases are different <quote>words</quote> to Simon.
This is an important design decision to allow more control when using a sophisticated grammar.
</para>

<para>
In general, it is advisable to keep your vocabulary as sleek as possible. The more words, the higher the chance that Simon might misunderstand you.
</para>

<para>
Example vocabulary (please note that the categories here are deliberately set to Noun / Verb to help the understanding; please refer to the <link linkend="grammar">grammar section</link> why this might not be the best idea):
<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Category</entry>
  <entry>Pronunciation</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
  <entry>k ax m p y uw t er</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
  <entry>ih n t er n eh t</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
  <entry>m ey l</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
  <entry>k l ow s</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<sect4 id="active_dictionary">
<title>Active Dictionary</title>
<para>The vocabulary used for the recognition is referred to as active dictionary or active vocabulary.</para>
</sect4>

<sect4 id="shadow_dictionary">
<title>Shadow Dictionary</title>
<para>
As said above, the user should keep his vocabulary / dictionary as lean as possible. However, as a word in your vocabulary has to also have information about its pronunciation, it would also be good to have a large dictionary where you could look up the pronunciation and other characteristics of the words.
</para>
<para>
Simon provides this functionality. We refer to this large reference dictionary as <quote>shadow dictionary</quote>. This shadow dictionary is not created by the user but can be imported from various sources.
</para>
<para>
As Simon is a multi-language solution we do not ship shadow dictionaries with Simon. However, it is very easy to import them yourself using the import dictionary wizard. This is described in the <link linkend="import_dictionary">Import Dictionary section</link>.
</para>
</sect4>
<sect4 id="language_profile_general">
<title>Language profile</title>
<para>
Additionally to a shadow dictionary, Simon can use a language profile to provide help with transcribing words.
</para>

<para>A language profile consists of rules how words are pronounced in the target language. It can be likened to the way that humans can often pronounce a word they never heard just because they know some implicit "pronunciation rules" of the language.</para>

<para>Just as with humans, this process is not perfect but can provide a solid starting ground.</para>

<para>This automatic deduction of a phoneme transcription from a written word is called "grapheme to phoneme conversion".</para>

<para>Simon requires the <ulink url="http://www-i6.informatik.rwth-aachen.de/web/Software/g2p.html">Sequitur G2P</ulink> grapheme to phoneme converter to be installed and set up for language profiles to work.</para>

<para>If you have <link linkend="configure_language_profile">selected a pre-built language profile</link> or <link linkend="create_language_profile">built your own</link>, Simon will automatically transcribe new words with it when they are not found in your <link linkend="shadow_dictionary">shadow dictionary</link>.</para>

</sect4>
</sect3>

<sect3 id="grammar_general">
<title>Grammar</title>
<para>
The grammar defines which combinations of words are correct.
</para>
<para>
Let's look at an example: You want to use Simon to launch programs and close those windows when you are done. You would like to use the following commands:
<itemizedlist>
  <listitem><para><quote>Computer, Internet</quote> to open a browser</para></listitem>
  <listitem><para><quote>Computer, Mail</quote></para><para>To open a mail client</para></listitem>
  <listitem><para><quote>Computer, close</quote></para><para>To close the current window</para></listitem>
</itemizedlist>
</para>
<para>
Following English grammar, your vocabulary would contain the following:

<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Category</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
To allow the sentences defined above Simon would need the following grammar:
<itemizedlist>
  <listitem><para><quote>Noun Noun</quote> for sentences like <quote>Computer Internet</quote></para></listitem>
  <listitem><para><quote>Noun Verb</quote> for sentences like <quote>Computer close</quote></para></listitem>
</itemizedlist>
</para>
<para>
While this would work, it would also allow the combinations <quote>Computer Computer</quote>, <quote>Internet Computer</quote>, <quote>Internet Internet</quote>, &etc; which are obviously bogus.
To improve the recognition accuracy, we can try to create a grammar that better reflects what we are trying to do with Simon.
</para>

<para>
It is important to remember that you define your own <quote>language</quote> when using Simon. That means that you are not bound to grammar rules that exist in whatever language you want to use Simon with. For a simple command and control use-case it would for example be advisable to invent new grammatical rules to eliminate the differences between different commands imposed by grammatical information not relevant for this use case.
</para>

<para>
In the example above it is for example not relevant that <quote>close</quote> is a verb or that <quote>Computer</quote> and <quote>Internet</quote> are nouns. Instead, why not define them as something that better reflects what we want them to be:

<table frame='all'><title>Improved Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Category</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Trigger</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Command</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
Now we change the grammar to the following:
<itemizedlist>
  <listitem><para><quote>Trigger Command</quote></para></listitem>
</itemizedlist>

This allows all the combinations described above. However, it also limits the possibilities to exactly those three sentences. Especially in larger models a well-thought-out grammar and vocabulary can mean a huge difference in recognition results.
</para>
</sect3>


</sect2>

<sect2 id="acoustic_model">
<title>Acoustic Model</title>
<para>The acoustic model represents your pronunciation in a machine readable format.</para>

<para>Let's look at the following sample vocabulary:
<table frame='all'><title>Sample Vocabulary</title>
<tgroup cols='3' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<colspec colname='c3'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Category</entry>
  <entry>Pronunciation</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Noun</entry>
  <entry>k ax m p y uw t er</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Noun</entry>
  <entry>ih n t er n eh t</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Noun</entry>
  <entry>m ey l</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Verb</entry>
  <entry>k l ow s</entry>
</row>
</tbody>
</tgroup>
</table>
</para>

<para>
The pronunciation of each word is composed of individual sounds which are separated by spaces. For example, the word <quote>close</quote> consists of the following sounds:

<itemizedlist>
  <listitem><para>k</para></listitem>
  <listitem><para>l</para></listitem>
  <listitem><para>ow</para></listitem>
  <listitem><para>s</para></listitem>
</itemizedlist>

The acoustic model uses the fact that spoken words are composed of sounds much like written words are composed of letters. Using this knowledge, we can segment words into sounds (represented by the pronunciation) and assemble them back when recognizing. These building blocks are called <quote>phonemes</quote>.
</para>

<para>
Because the acoustic model actually represents how you speak the phonemes of the words, training material is shared among all words that use the same phonemes.
</para>
<para>
That means if you add the word <quote>clothes</quote> to the language model, your acoustic model already has an idea how the <quote>clo</quote> part is going to sound as they share the same phonemes (<quote>k</quote>, <quote>l</quote>, <quote>ow</quote>) at the beginning.
</para>
<para>
To train the acoustic model (in other words to tell him how you pronounce the phonemes) you have to <quote>train</quote> words from your language model. That means that Simon displays a word which you read out loud. Because the word is listed in your vocabulary, Simon already knows what phonemes it contains and can thus <quote>learn</quote> from your pronunciation of the word.
</para>
</sect2>


</sect1>


<sect1 id="scenarios_use_advanced">
<title>Scenarios</title>
<para>This section extends the previous one about <link linkend="scenarios_use">basic scenario management</link> and tells you how to create, edit and export scenarios.</para>

<para>
  <screenshot>
    <screeninfo>Manage scenarios</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="manage_scenarios_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<sect2 id="scenario_hierarchies">
<title>Scenario hierarchies</title>

<para>You can create scenario hierarchies by dragging and dropping active scenarios on top of each other.</para>

<para>
  <screenshot>
    <screeninfo>Manage scenarios: Scenario hierarchies</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="manage_scenarios_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Scenario hierarchies serve two purposes:
<itemizedlist>
<listitem><para>The <link linkend="context">context system</link> respects scenario hierarchies: If the parent scenario gets deactivated, all child scenarios will become deactivated as well.</para></listitem>
<listitem><para>If you attempt to <link linkend="export_scenario">export a scenario</link> that has children, &kmyapplication; will allow you to export them in a joint scenario package. This way, you can share multiple logically co-dependent scenarios (&eg; one "Office" scenario that contains sub-scenarios for "Word", "Excel", etc.).</para></listitem>
</itemizedlist>
</para>

</sect2>

<sect2 id="add_scenario">
  <title>Adding a new Scenario</title>
  <para>To add a new scenario, select the <guibutton>Add</guibutton> button. A new dialog will be displayed.</para>

<para>
  <screenshot>
    <screeninfo>Add scenario</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="add_scenario.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>When creating a new scenario, please give it a descriptive name. For the later upload on <ulink url="https://store.kde.org/browse/cat/319/ord/latest/">KDE Store</ulink> we would kindly ask you to follow a certain naming scheme although this is of course not a requirement: <quote>[&lt;language&gt;/&lt;base model&gt;] &lt;name&gt;</quote>. If, for example you create a scenario in English that works with the Voxforge base model and controls Mozilla Firefox this becomes: <quote>[EN/VF] Firefox</quote>. If your scenario is not specifically tailored to one phoneme set (base model), just omit the second tag like this: <quote>[EN] Firefox</quote>.</para>

<para>The scenario version is just an incremental version number that makes it easier to distinguish between different revisions of a scenario.</para>

<para>If your scenario needs a specific feature of Simon (for example because you use a new plugin), you can define minimum and maximum version numbers of Simon here.</para>

<para>The license of your scenario can be set through the drop down. You can of course also add an arbitrary license text directly in the input field.</para>

<para>You can then add your name (or alias) to the list of scenario authors. There you will also be asked for contact information. This field is purely provided as a convenient way to contact a scenario author for changes, problems, fanmail &etc; If you don't feel comfortable providing your email address you can simply enter a dash <quote>-</quote> denoting that you are not willing to divulge this information.</para>

</sect2>

<sect2 id="edit_scenario">
<title>Edit Scenario</title>
<para>To edit scenarios, just select <guibutton>Edit</guibutton> from the <guilabel>Manage scenarios</guilabel> dialog.</para>
<para>The dialog works exactly the same as the <link linkend="add_scenario">add scenario</link> dialog.</para>
</sect2>

<sect2 id="export_scenario">
  <title>Export Scenario</title>
  <para>Scenarios can be exported to a local file in Simon's XML scenario file format and directly uploaded to the <ulink url="http://kde-files.org/index.php?xcontentmode=692">Simon Scenarios</ulink> subsection of the OpenDesktop site <ulink url="http://kde-files.org">KDE-files.org</ulink>.</para>

  <para>To upload to OpenDesktop sites, you need an account on the site. <ulink url="http://opendesktop.org/usermanager/new.php">Registration</ulink> is very easy and of course free of charge.</para>

  <para>Simon allows you to upload new content directly from within Simon (<guibutton>Export</guibutton> > <guimenuitem>Publish</guimenuitem>).</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 1 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 2 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 3 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  <screenshot>
    <screeninfo>Upload scenario wizard: 4 of 4</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="upload_scenario_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

  <para>To use this functionality, simply enter your account credentials in the <link linkend="social_desktop_config">social desktop configuration</link> in the Simon configuration.</para>

</sect2>
</sect1>


<sect1 id="vocabulary">
<title>Vocabulary</title>

<para>The vocabulary module defines the set of words of the scenario.

<screenshot>
<screeninfo>Simon's Vocabulary</screeninfo>
<mediaobject>
<imageobject>
  <imagedata fileref="vocabulary.png"   format="PNG"/>
</imageobject>
</mediaobject>
</screenshot>
</para>

<para>Per default, the active vocabulary is shown. To display the shadow vocabulary select the tab <guilabel>Shadow Vocabulary</guilabel>.</para>

<para>
Every word states it <quote>recognition rate</quote> which at the moment is just a counter of how often the word has been recorded (alone or together with other words).
</para>

<para>
<screenshot>
<screeninfo>Shadow Vocabulary</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="vocabulary1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>


</para>

<sect2 id="add_word">
<title>Adding Words</title>
<para>To add new words to the active vocabulary, use the add word wizard.</para>
<para>Adding words to Simon is basically a two step procedure:
<itemizedlist>
<listitem><para>Defining the word</para></listitem>
<listitem><para>Initial training</para></listitem>
</itemizedlist>
</para>

<sect3 id="add_word_define">
<title>Defining the Word</title>
<para>
Firstly, the user is asked which word he wants to add.

<screenshot>
<screeninfo>Select the word to add</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
When the user proceeds to the next page, Simon automatically tries to find as much information about the word in the <link linkend="shadow_dictionary">shadow dictionary</link> as possible.
</para>

<para>
If the word is listed in the shadow dictionary, Simon automatically fills out all the needed fields (Category and Pronunciation).

<screenshot>
<screeninfo>Fields automatically filled out by the Shadow Dictionary</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para> All suggestions from the shadow dictionary are listed in the table <guilabel>Similar words</guilabel>. Per default only exact word matches are shown. However, this can be changed by checking the <guilabel>Include similar words</guilabel> check box below the suggestion table. Using similar words you can quickly deduce the correct pronunciation of the word you are actually trying to add. See <link linkend="manual_transcription">below</link> for details.</para>

<para>
Of course this really depends on your shadow dictionary. If the shadow dictionary does not contain the word you are trying to add, the required fields have to be filled out manually.</para>

<para>
Some dictionaries that can be imported with Simon (SPHINX, HTK) do not differentiate between upper and lower case. Suggestions based on those dictionaries will always be uppercase. You are of course free to change these suggestions to the correct case.
</para>

<para>Some dictionaries that can be imported with Simon (SPHINX, PLS and HTK) provide no grammatical information at all. These will assign all the words to the category <guilabel>Unknown</guilabel>. You should change this to something appropriate when adding those words.
</para>

<sect4>
<title>Manually Selecting a Category</title>
<para>
The category of the word is defined as the grammatical category the word belongs to. This might be <guilabel>Noun</guilabel>, <guilabel>Verb</guilabel> or completely new categories like <guilabel>Command</guilabel>. For more information see the <link linkend="grammar">grammar section</link>.</para>
<para>
The list contains all categories used in both your active and your shadow lexicon and in your grammar.
</para>
<para>
You can add new categories to the drop-down menu by using the green plus sign next to it.
</para>
</sect4>

<sect4 id="manual_transcription">
<title>Manually Providing the Phonetic Transcription</title>
<para>
The pronunciation is a bit trickier. Simon does not need a certain type of phonetics so you are free to use any method as long as it uses only ASCII characters and no numbers. However, if you want to use a shadow dictionary and want to use it to its full potential you should use the same phonetics as the shadow dictionary.
</para>

<para>
If you do not know how to transcribe a word yourself you can easily use your shadow dictionary to help you with the transcription - even if the word is not listed in it. Let's say we want to add the word <quote>Firefox</quote> (to launch firefox) which is of course not listed in our shadow dictionary.
</para>

<para>
(I imported the English voxforge HTK lexicon available from <ulink url="http://voxforge.org/home/downloads">voxforge</ulink> as a shadow dictionary.)
</para>

<para>
<quote>Firefox</quote> is not listed in our shadow dictionary so we do not get any suggestion at all.
<screenshot>
<screeninfo>Adding an unknown word</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
However, we know that firefox sounds like <quote>fire</quote> and <quote>fox</quote> put together. So let's just open the vocabulary (you can keep the wizard open) by selecting <guibutton>Vocabulary</guibutton> from your Simon main toolbar.
</para>
<para>
  Switch to the shadow vocabulary by clicking on the tab <guilabel>Shadow Vocabulary</guilabel>.</para>
<para>
Use the <guilabel>Filter</guilabel> box above the list to search for <quote>Fire</quote>:

<screenshot>
<screeninfo>Adding an unknown word: Search for the Pronunciation</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
We can see, that the word <quote>Fire</quote> is transcribed as <quote>f ay r</quote>. Now filter for <quote>fox</quote> instead of <quote>Fire</quote> and we can see that <quote>Fox</quote> is transcribed as <quote>f ao k s</quote>. We can assume, that firefox should be transcribed as <quote>f ay r f ao k s</quote>.
</para>

<para>
Using this approach of deducing the pronunciation from parts of the word has the distinct advantage that we not only get a high quality transcription but also automatically use the same phoneme set as the other words which were correctly pulled out of the shadow dictionary.
</para>

<para>
We can now enter the pronunciation and change the category to something appropriate.
<screenshot>
<screeninfo>Completely defined word</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_2_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect4>

</sect3>

<sect3 id="add_word_record">
<title>Training the Word</title>
<para>
  To complete the wizard we can now train the word twice. If you don't want to do this or for example use a <link linkend="base_model_static">static base model</link>, you can skip these two pages.
</para>

<para>
  Because you are about to record some training samples, Simon will display the volume calibration to make sure that your microphone is set up correctly. For more information please refer to the <link linkend="volume_calibration">volume calibration section</link>
</para>

<para>
Simon will try to prompt you for real-world examples. To do that, Simon will automatically fetch grammar structures using the category of the word and substitute the generic categories with example words from your active lexicon.
</para>
<para>
For example: You have the grammar structure <quote>Trigger Command</quote> and have the word <quote>Computer</quote> of the category <quote>Trigger</quote> in your vocabulary. You then add a new word <quote>Firefox</quote> of the category <quote>Command</quote>. Simon will now automatically prompt you for <quote>Computer Firefox</quote> as it is - according to your grammar - a valid sentence.
</para>
<para>
If Simon is unable to find appropriate sentences using the word (&ie;: No grammar, not enough words in your active lexicon, &etc;) it will just prompt you for the word alone.
</para>
<para>
Although Simon ensures that the automatically generated examples are valid, you can always override its suggestions. Just switch to the <guilabel>Examples</guilabel> tab on the <guilabel>Define Word</guilabel> page.
<screenshot>
<screeninfo>Editing word examples</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
You are free to change those examples to anything you like. You can even go so far and use words that are not yet in your active lexicon as long as you add them before you synchronize the model, although this is not recommended.
</para>

<para>
All that is left is to record the examples.
<screenshot>
<screeninfo>Recording</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="add_word_4.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>Make sure you follow the guidelines listed in the <link linkend="recording">recording section</link>.</para>
</sect3>

</sect2>

<sect2 id="edit_word">
<title>Editing a word</title>
<para>
  To edit a word, simply select it from the vocabulary, and click on <guibutton>Edit</guibutton>.
</para>

<para>
  <screenshot>
    <screeninfo>Edit word</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="edit_word.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>
  There you can change name, category and pronunciation of the selected word.
</para>
</sect2>

<sect2 id="remove_word">
<title>Removing a word</title>

<para>To remove a word from your language model, select it in the vocabulary view and click on <guibutton>Remove</guibutton>.

<screenshot>
<screeninfo>Recording</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="delete_word.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot></para>

<para>
The dialog offers four choices:
<itemizedlist>
  <listitem>
    <para>Move the word to the <guilabel>Unused</guilabel> category.</para>
    <para>Because you (hopefully) don't use the category <guilabel>Unused</guilabel> in your grammar, the word will no longer be considered for recognition. In fact, it will be removed from the active vocabulary before compiling the model because no grammar sentence references it.</para>
    <para>If you want to use the category <guilabel>Unused</guilabel> in your grammar, you can of course use a different category for unused words. Just set the category through the <link linkend="edit_word">Edit word</link> dialog.</para>
    <para>To use the word again, just set the right category again. No data will be lost.</para>
  </listitem>
  <listitem>
    <para>Move the word to the shadow lexicon</para>
    <para>This will remove the selected word from the active lexicon (and thus from the recognition) but will keep a copy in the shadow vocabulary. All the recordings containing the word will be preserved.</para>
    <para>To use the word again, add it again to the active vocabulary. When adding a <quote>new</quote> word with the same name the values of the moved word will be suggested to you. Therefore, no data will be lost.</para>
  </listitem>
  <listitem>
    <para>Delete the word but keep the samples</para>
    <para>Removes the word completely but keeps the associated samples. Whenever you add another word with the same word name the samples will be re-associated.</para>
    <para>Be careful with this option as the new word you add again might be transcribed differently and this difference cannot be taken into account automatically (Simon will then try to force the new transcription on the old recordings during the model compilation).</para>
    <para>Do not use this option if the samples you recorded for this word were erroneous.</para>
  </listitem>
  <listitem><para>Remove the word completely</para>
  <para>Just remove the word. All the recordings containing the word will be removed too.</para>
  <para>This option leaves no trace of neither the word itself nor the associated samples.</para>
  <para>Because samples are global (not assigned to scenarios), even samples recorded from training sessions of other scenarios might be removed as well if they contain the word. Use this option carefully.</para>
  </listitem>
</itemizedlist>
</para>

</sect2>

<sect2 id="special_training_stub">
<title>Special Training</title>
<para>Please see the <link linkend="special_training">special training section in the training section</link>.</para>
</sect2>


<sect2 id="import_dictionary">
<title>Importing a Dictionary</title>
<para>
Simon provides the functionality to import large dictionaries as a reference. This reference dictionary is called <link linkend="shadow_dictionary">shadow dictionary</link>.
</para>
<para>
When the user <link linkend="add_word">adds a new word</link> to the model, he has to define the following characteristics to define this word:
<itemizedlist>
  <listitem><para>Wordname</para></listitem>
  <listitem><para>Category</para></listitem>
  <listitem><para>Phonetic definition</para></listitem>
</itemizedlist>
</para>
<para>
These characteristics are taken out of the shadow dictionary if it contains the word in question. A large, high quality shadow dictionary can thus help the user to easily add new words to the model without keeping track of the phoneme set or - in many cases - even let him forget that the phonetic transcription is needed at all.
</para>

<para>
  <screenshot>
    <screeninfo>Import dictionary: Introduction</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="import_dictionary_0.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Since version 0.3 you can also import dictionaries directly to the active dictionary. This option is mostly there to make it easier to move to Simon from custom solutions and to encourage importing of older models (for example one used with Simon 0.2). You will <emphasis>almost never</emphasis> want to import a very large dictionary as active dictionary.</para>

<para>You can find a list of available dictionaries that work with Simon on the <ulink url="http://userbase.kde.org/Special:myLanguage/Simon/Shadow_dictionary">Simon wiki</ulink>.</para>

<para>
Simon is able to import five different types of dictionaries:
<itemizedlist>
  <listitem><para>HADIFIX</para></listitem>
  <listitem><para>HTK</para></listitem>
  <listitem><para>PLS</para></listitem>
  <listitem><para>SPHINX</para></listitem>
  <listitem><para>Julius</para></listitem>
</itemizedlist>
</para>



<sect3 id="import_dict_hadifix">
<title>HADIFIX Dictionary</title>
<para>
Simon can import HADIFIX dictionaries.
</para>
<para>
One example of a HADIFIX dictionary is the German <ulink url="http://www.sk.uni-bonn.de/forschung/phonetik/sprachsynthese/bomp">HADIFIX BOMP</ulink>.
</para>
<para>
Hadifix dictionaries provide both categories and pronunciation.
</para>

<para>
Due to a special exemption in their license the Simon listens team is proud to be able to offer you to download the excellent HADIFIX BOMP directly from within Simon.
</para>


<para>
  <screenshot>
    <screeninfo>Import dictionary: Automatic BOMP import</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="import_dictionary_hadifix_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
</para>

<para>Using the automatic bomp import you can, after providing name and email address for the team of the University Bonn, directly download and import the dictionary from the Simon listens server.</para>
</sect3>

<sect3 id="import_dict_htk">
<title>HTK Dictionary</title>
<para>
Simon can import HTK lexica.
</para>
<para>
One example of a HTK lexicon is the English <ulink url="http://www.repository.voxforge1.org/downloads/SpeechCorpus/Trunk/Lexicon/">Voxforge dictionary</ulink>.
</para>
<para>
Hadifix dictionaries provide pronunciation information but no categories. All words will be assigned to the category <guilabel>Unknown</guilabel>.
</para>
</sect3>

<sect3 id="import_dict_pls">
<title>PLS Dictionary</title>
<para>
Simon can import PLS dictionaries.
</para>
<para>
One example of a PLS dictionary is the <ulink url="http://www.repository.voxforge1.org/downloads/de/Trunk/Lexicon/">German GPL dictionary from Voxforge</ulink>.
</para>
<para>
PLS dictionaries provide pronunciation information but no categories. All words will be assigned to the category <guilabel>Unknown</guilabel>.
</para>
</sect3>

<sect3 id="import_dict_sphinx">
<title>SPHINX Dictionary</title>
<para>
Simon can import SPHINX dictionaries.
</para>
<para>
One example of a SPHINX dictionary is this <ulink url="http://speech.mty.itesm.mx/~jnolazco/proyectos.htm">dictionary for Mexican Spanish</ulink>.
</para>
<para>
SPHINX dictionaries provide pronunciation information but no categories. All words will be assigned to the category <guilabel>Unknown</guilabel>.
</para>
</sect3>

<sect3 id="import_dict_julius">
<title>Julius Dictionary</title>
<para>
Simon can import Julius vocabularies.
</para>
<para>
One example of a Julius vocabularies are the word lists of Simon 0.2.
</para>
<para>
Julius dictionaries provide pronunciation information as well as category information.
</para>
</sect3>
</sect2> <!-- End Import Dict -->

<sect2 id="create_language_profile">
<title>Create language profile</title>
<para>Here, you can build a <link linkend="language_profile_general">language profile</link> from your shadow dictionary.</para>

<para>
<screenshot>
<screeninfo>Create language profile</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="create_language_profile.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>After selecting <guibutton>Create profile</guibutton>, &kmyapplication; will analyze your current shadow dictionary and try to deduce the transcription rules from it.</para>
<para>This is generally a very length process and can, depending on the size of your shadow dictionary, take up to several hours.</para>
<para>The created profile will be selected automatically after the process completes.</para>

</sect2>

</sect1> <!-- End Vocabulary -->


<sect1 id="grammar">
<title>Grammar</title>

<para>
Simon provides an easy to use text based interface to change the grammar. You can simply list all the allowed sentences (without any punctuation marks, obviously) like described above.

<screenshot>
<screeninfo>Grammar</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="grammar.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>When selecting a sentence on the left, the right pane will automatically show possible real sentences with the words of your vocabulary on the right.</para>

<para>The example section will list at most 35 examples so if more than that amount of sentences match the selected grammar entry, the list might not be complete.</para>

<sect2>
<title>Import a Grammar</title>
<para>Additionally to simply entering your desired grammar sentence by sentence, Simon is able to automatically deduce allowed grammar structures by reading plain text using the Import Grammar wizard.

<screenshot>
<screeninfo>Import Grammar</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Simon can read and import text files but also provides an input field if you want to simply type the text into Simon.</para>

<para>Say we have a vocabulary like in the general section above:

<table frame='all'><title>Improved Sample Vocabulary</title>
<tgroup cols='2' align='left' colsep='1' rowsep='1'>
<colspec colname='c1'/>
<colspec colname='c2'/>
<thead>
<row>
  <entry>Word</entry>
  <entry>Category</entry>
</row>
</thead>
<tbody>
<row>
  <entry>Computer</entry>
  <entry>Trigger</entry>
</row>
<row>
  <entry>Internet</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>Mail</entry>
  <entry>Command</entry>
</row>
<row>
  <entry>close</entry>
  <entry>Command</entry>
</row>
</tbody>
</tgroup>
</table></para>

<para>We want Simon to recognize the sentence <quote>Computer Internet!</quote>. So we either enter the text using the <guibutton>Import text</guibutton> option or create a simple text file with this content <quote>Computer Internet!</quote> (any punctuation mark would work) and save it as <filename>simongrammar.txt</filename> to use the <guibutton>Import files</guibutton> option.

<screenshot>
<screeninfo>Import Grammar: Enter text</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<screenshot>
<screeninfo>Import Grammar: Text file</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<screenshot>
<screeninfo>Import Grammar: Select files</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_grammar3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Simon will then read the entered text or all the given text files (in this case the only given text file is <filename>simongrammar.txt</filename>) and look up every single word in both active and shadow dictionary (the definition in the active dictionary has more importance if the word is available in both). It will then replace the word with its category.</para>
<para>In our example this would mean that he would find the sentence <quote>Computer Internet</quote>. Simon would find out that <quote>Computer</quote> is of the category <guilabel>Trigger</guilabel> and <quote>Internet</quote> of the category <guilabel>Command</guilabel>. Because of this Simon would <quote>learn</quote> that <quote>Trigger Command</quote> is a valid sentence and add it to its grammar.</para>
<para>The import automatically segments the input text by punctuation marks (<quote>.</quote>, <quote>-</quote>, <quote>!</quote>, &etc;) so any natural text should work. The importer will automatically merge duplicate sentence structures (even across different files) and add multiple sentence (all possible combinations) when a word has multiple categories assigned to it.</para>
<para>The import will ignore sentences where one or more words could not be found in the language model unless you tick the <guilabel>Also import unknown sentences</guilabel> check box in which case those words are replaced with <guilabel>Unknown</guilabel>.</para>

</sect2>





<sect2>
<title>Renaming Categories</title>
<para>The rename category wizard allows you to rename categories in both your active vocabulary, your shadow dictionary and the grammar.

<screenshot>
<screeninfo>Rename Category</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="rename_category.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2>
<title>Merging Categories</title>
<para>The merge category wizard allows you to merge two categories into one new category in both your active vocabulary, your shadow dictionary and the grammar.

<screenshot>
<screeninfo>Merge Category</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="merge_category.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>This functionality is especially useful if you want to simplify your grammar structures.</para>
</sect2>


</sect1>



<sect1 id="training">
<title>Training</title>

<para>Using the Training-module, you can improve your <link linkend="acoustic_model">acoustic model</link>. </para>

<para>
The interface lists all installed training texts in a table with three columns:
<itemizedlist>
<listitem><para>Name</para><para>A descriptive name for the text.</para></listitem>
<listitem><para>Pages</para><para>The number of <quote>pages</quote> the text consists of. Each page represents one recording.</para></listitem>
<listitem><para>Recognition Rate</para><para>Analogue to the vocabulary; represents how likely Simon will recognize the words (higher is better). The recognition rate of the training text is the average recognition rate of all the words in the text.</para></listitem>
</itemizedlist>

<screenshot>
<screeninfo>Training</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
To improve the acoustic model - and thus the recognition rate - you have to record training texts. This means that Simon gets essentially two needed parts:
</para>

<itemizedlist>
  <listitem><para>Samples of your speech</para></listitem>
  <listitem><para>Transcriptions of those samples</para></listitem>
</itemizedlist>

<para>
The active dictionary is used to transcribe the words (mapping them from the actual word to its phonetic transcription) that make up the text so every word contained in the training text you want to read (train) has to be contained in your active dictionary. Simon will warn you if this is not the case and provide you with the possibility to add all the missing words in one go.
</para>

<screenshot>
<screeninfo>Training: Warning about missing words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_warning.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>
The procedure is the same as if you would <link linkend="add_word">add a single word</link> but the wizard will prompt you for details and recordings for all the missing words automatically. This procedure can be aborted at any time and Simon will provide both a way to add the already completely defined words and to undo all changes done so far. When the user has added all the words he is prompted for (all the words missing) the changes to the active dictionary / vocabulary are saved and the training of the previously selected text starts automatically.
</para>

<para>
<anchor id="training_in_progress" />
The training (reading) of the training text works exactly the same as the initial training when adding a new word.</para>

<para>Make sure you follow the guidelines listed in the <link linkend="recording">recording section</link>.</para>
<para>
<screenshot>
<screeninfo>Training in progress</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="training_running.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>



<sect2 id="training_storage">
<title>Storage Directories</title>
<para>
Training texts are stored in two different locations:
<itemizedlist>
  <listitem><para>Linux: <filename>~/.kde/share/apps/simon/texts</filename></para>
            <para>Windows: <filename>%appdata%\.kde\share\apps\simon\texts</filename></para>
          <para>The texts of the current user. Can be deleted and added with Simon (see below).</para></listitem>
  <listitem><para>Linux: <filename>`kde4-config --prefix`/share/apps/simon/texts</filename></para>
            <para>Windows: <filename>(install folder)\share\apps\simon\texts</filename></para>
          <para>System-wide texts. They will appear on every user account using Simon on this machine and cannot be deleted from within Simon because of the obvious permission restrictions on system-wide files.</para>
          <para>This folder can be used by system administrators to provide a common set of training texts for all the users on one system.</para>
          </listitem>
</itemizedlist>
</para>
<para>The XML files (one for each text) can just be moved from one location to the other but this will most likely require admin privileges.</para>
</sect2>


<sect2 id="add_texts">
<title>Adding Texts</title>
<para>
<screenshot>
<screeninfo>Import-training-texts-wizard</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="import_text_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

The add texts wizard provides a simple way to add new training texts to Simon.
</para>

<para>When importing text files, Simon will automatically try to recognize individual sentences and split the text into appropriate <quote>pages</quote> (recordings). The algorithm treats text between <quote>normal</quote> punctuation (<quote>.</quote>, <quote>!</quote>, <quote>?</quote>, <quote>...</quote>, <quote>"</quote>,...) and line breaks as <quote>sentences</quote>. Each <quote>sentence</quote> will be on its own page.</para>

<para>
  Simon supports two different sources for new training texts.
</para>

<sect3>
  <title>Add training texts</title>
            <para>
      <screenshot>
      <screeninfo>Import-training-texts-wizard: Add training texts</screeninfo>
        <mediaobject>
          <imageobject>
            <imagedata fileref="import_text_4.png" format="PNG"/>
          </imageobject>
        </mediaobject>
      </screenshot>

        Simply enter the training text in an input field.</para>
</sect3>
<sect3>
  <title>Local text files</title>

    <para>
      <screenshot>
      <screeninfo>Import-training-texts-wizard: Local text files</screeninfo>
        <mediaobject>
          <imageobject>
            <imagedata fileref="import_text_3.png" format="PNG"/>
          </imageobject>
        </mediaobject>
      </screenshot>

    Simon can import normal text files to use them as training texts.</para>
</sect3>

</sect2>

<sect2 id="special_training">
<title>On-The-Fly Training</title>
<para>In addition to training texts, Simon also allows to train individual words or word combinations from your dictionary on-the-fly.</para>

<para>This feature is located in the vocabulary menu of Simon.</para>

<para>
<screenshot>
<screeninfo>Special Training: Selecting the Words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="special_training.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Select the words to train from the vocabulary on the left and simply drag them to the selection list to the right (you could also select them in the table on the left and add them by clicking <guibutton>Add to Training</guibutton>).</para>

<para>Start the training by selecting <guibutton>Train selected words</guibutton>. The training itself is exactly the same as if it were a pre-composed training text.</para>

<para>
<screenshot>
<screeninfo>Special Training: Training the Words</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="special_training_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>If there are more than 9 words to train Simon will automatically split the text evenly across multiple pages.</para>

<para>Of course you are free to add words from the shadow lexicon to the list of words to train but Simon will prompt you to add the words before the training starts just like he would if you would train a text that contains unknown words (see above).</para>
</sect2>

</sect1>

<sect1 id="context">
<title>Context</title>
<para>&kmyapplication; includes a context layer that allows you to let Simon automatically adjust its configuration depending on its context.</para>

<para>For example, you could set up &kmyapplication; to only allow commands like "New tab" if Mozilla Firefox is running and the currently active window.</para>

<para>There are three major areas that contextual information can influence:

<itemizedlist>
<listitem><para>Scenario selection</para></listitem>
<listitem><para>Sample groups</para></listitem>
<listitem><para>Active microphones</para></listitem>
</itemizedlist>
</para>

<sect2 id="context_scenarios">
<title>Scenario selection</title>
<para>Scenarios can specify to only be active during certain contextual situations. If these situations are not met, &kmyapplication; will temporarily deactivate the affected scenario.</para>

<para>
<screenshot>
<screeninfo>Scenario context</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="context.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The local context conditions of this scenario are shown in the list of <guilabel>Activation Requirements</guilabel> and can be added, edited and deleted through the respective buttons.</para>
<para>The context conditions respect a possible <link linkend="scenario_hierarchies">hierarchy of scenarios</link>: The activation requirements of all direct or indirect parent scenarios also apply to the child scenario(s). This condition "inheritance" is shown on the right side.</para>

<para>The &kmyapplication; main window also shows a list of currently used scenarios. Scenarios that are deactivated because of their activation requirements (context conditions) are listed in light gray and italic. The screenshot below, for example, shows a temporarily deactivated Amarok scenario.</para>

<para>
<screenshot>
<screeninfo>Identifying deactivated scenarios</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="context_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The same visual hints (gray, italic font for unmet activation criteria) also apply to the individual context conditions in the context menu.</para>
</sect2>

<sect2 id="sample_group">
<title>Sample groups</title>
<para>Every sample recorded with Simon is assigned a sample group. Sample groups can be configured to only be used for the building of the acoustic models if certain contextual conditions are met. If this is not the case, all samples tagged with the deactivated sample group will be temporarily removed from the training corpus.</para>

<para>For more information, an example use-case and instructions on how to work with sample groups, please refer to the section on <link linkend="configuration_sound_context">sample groups</link>.</para>
</sect2>

<sect2 id="context_conditions">
<title>Context conditions</title>

<para>In Simon, context is monitored through a set of context condition plugins.</para>
<para>In general, context conditions are combined through an "and" association. For example, if the activation of resource is bound by two conditions A and B, it will only be activated if both A and B see their conditions met. To instead model alternatives ("A or B or both"), use an <link linkend="or_condition_association">Or Condition Association</link>.</para>

<para>All conditions can optionally be inverted. Inverting a condition means that it will evaluate to true if it would otherwise evaluate to false and vice versa.</para>

<sect3 id="active_window_condition">
<title>Active window</title>
<para>True, if the title of the currently active foreground window matches the provided window title.</para>

<para>
<screenshot>
<screeninfo>Active window condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="active_window_condition.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="dbus_condition">
<title>D-Bus</title>
<para>The D-Bus condition plugin allows to monitor 3rd party applications that export state information on D-Bus.</para>

<para>The monitored application needs to provide two methods: One signal to notify of changes and another method that returns the current state.</para>

<para>
<screenshot>
<screeninfo>D-Bus condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="dbus_condition1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The screenshot above, for example, configures a D-Bus condition that will evaluate to true while the music player "Tomahawk" is playing and to false otherwise.</para>
</sect3>

<sect3 id="face_detection_condition">
<title>Face detection</title>

<para>The face detection condition will evaluate to true, if &kmyapplication;'s vision layer has identified a person sitting in front of the <link linkend="webcam_configuration">configured webcam</link>.</para>

<para>
<screenshot>
<screeninfo>Face detection condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="face_detection_condition.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="file_content_condition">
<title>File content</title>
<para>This condition plugin will return true, if the given file contains the provided content.</para>

<para>The file will be monitored for changes.</para>

<para>
<screenshot>
<screeninfo>File content condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="file_content_condition.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="lip_detection_condition">
<title>Lip detection</title>

<para>The lip detection condition will evaluate to true, if &kmyapplication;'s vision layer has identified a person sitting in front of the <link linkend="webcam_configuration">configured webcam</link> and is speaking something (lip movements). </para>

<para>
<screenshot>
<screeninfo>Lip detection condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="lip_detection_condition.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
<para>The lip detection training will try to determine the optimal value of sensitivity of the detection by monitoring your lip movements. For better accuracy of lip detection condition, stop training when the sensitivity value on the slider during training becomes almost constant.
</para>
</sect3>

<sect3 id="or_condition_association">
<title>Or condition association</title>
<para>The or condition association allows you to configure a meta-condition that reports to be satisfied as soon as one or more of its child conditions evaluates to true.</para>

<para>Or condition associations can have an arbitrary number of child conditions that may even also be or condition associations.</para>

<para>
<screenshot>
<screeninfo>Or condition association</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="or_condition_association.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="process_opened_condition">
<title>Process opened</title>

<para>Is satisfied if there is a running process with the provided executable name.</para>

<para>
<screenshot>
<screeninfo>Process opened condition</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="process_opened_condition.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

</sect2>
</sect1>

<sect1 id="commands">
<title>Commands</title>
<para>When Simon is active and recognizes something, the recognition result is given to the loaded command plug-ins (in order) for processing.</para>

<para>
<screenshot id="command_dialog">
<screeninfo>Simon's Command Dialog</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_main.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The command system can be compared with a group of factory workers. Each one of them knows how to perform one task (&eg; <quote>Karl</quote> knows how to start a program and <quote>Joe</quote> knows how to open a folder, &etc;). Whenever Simon recognizes something it is given to <quote>Karl</quote> who then checks if this instruction is meant for him. If he doesn't know what to do with it, it is handed over to <quote>Joe</quote> and so on. If none of the loaded plugins know how to process the input it is ignored. The order in which the recognition result is given to the individual commands (people) is configurable in the command options (<guibutton>Commands</guibutton> > <guibutton>Manage plugins</guibutton>).</para>


<para>
<screenshot>
<screeninfo>Simon's Action Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_actions_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Each plugin can be associated with a <quote>trigger</quote>. Using triggers, the responsibility of each plugin can be easily be divided.
</para>
<para>Using the factory workers abstraction from above it could be compared to stating the name of who you mean to process your request. So instead of <quote>Open my home folder</quote> you say <quote>Joe, open my home folder</quote> and <quote>Joe</quote> (the plugin responsible for opening folders) will instantly know that the request is meant for him.</para>
<para>In practice you could have commands like the executable command <quote>Firefox</quote> to open the popular browser and the place command <quote>Google</quote> to open the web search engine. If you assign the trigger <quote>Start</quote> to the executable plugin and the trigger <quote>Open</quote> to the place command you would have to say <quote>Start Firefox</quote> (instead of just <quote>Firefox</quote> if you don't use a trigger for the executable plugin) and <quote>Open Google</quote> to open the search engine (instead of just <quote>Google</quote>).</para>
<para>Triggers are of course no requirement and you can easily use Simon without defining any plugin triggers (although many plugins come with a default trigger of <quote>Computer</quote> set which you would have to remove). But even if you use just one trigger for all your commands (like <quote>Computer</quote> to say <quote>Computer, Firefox</quote> and <quote>Computer, Google</quote> like) it has the advantage of greatly limiting the number of false-positives.</para>

<para>Simon's <link linkend="command_dialog">command dialog</link> displays the complete phrase associated with a command in the upper right corner of the command configuration.</para>

<para>You can load multiple instances of one plugin even in one scenario. Each instance can of course also have a different plugin trigger.</para>

<para>
Each Command has a name (which will trigger its invocation), an icon and more fields depending on the type of the plugin (see below).
</para>

<para>
Some command plugins might provide a configuration of the plugin itself (not the commands it contains). These configuration pages will be plugged directly into the action configuration dialog (below the <guimenuitem>General</guimenuitem> menu item) when you load the associated plugin.
</para>

<para>
  Plugins that provide a graphical user interface (like for example the <link linkend="input_number">input number command plugin</link>) can be configured by configuring <guilabel>Voice commands</guilabel>. You can, for example, change the associated word that will trigger the button, but also change the displayed icon, &etc; If you remove all voice interface commands from a graphical element, the element will be hidden automatically.
</para>

<para>Voice interface commands are added just like normal commands through the command configuration.</para>

<para>
<screenshot>
<screeninfo>Configure voice interface commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="voice_interface_command_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To add a new interface command to a function, just select the action you want to associate with a command, click <guibutton>Create from Action template</guibutton> and adapt the resulting command to your needs.</para>

<para>
  Some plugins (for example the <link linkend="desktopgrid">desktop grid</link> or the <link linkend="calculator_command_plugin">calculator</link>) might also provide a menu item in the <guimenu>Actions</guimenu> menu.
</para>

<para>
<screenshot>
<screeninfo>Command plugged into main window</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_plug.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>Scenarios can optionally define one command that will immediately be run when the scenario is initialized. If you require more than one command to run automatically, consider the use of a <link linkend="composite_commands">composite command</link>.

<screenshot>
<screeninfo>Autorun command</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_autorun.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>


<anchor id="command_arguments" />
<para>Command triggers can contain placeholders in the form of "%&lt;index&gt;", referring to any one word, or "%%&lt;index&gt;" describing one or more left out words. For example the recognition result "Next window" will be matched by the triggers "Next %1", "Next %%1" and "%%1" but <emphasis>not</emphasis> by the triggers "%1", "Next window %1", "%%1 Next window".</para>

<sect2 id="executable_commands">
<title>Executable Commands</title>

<para>Executable commands are associated with an executable file (<quote>Program</quote>) which is started when the command is invoked.</para>

<screenshot>
<screeninfo>Executable Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_program.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>Arguments to the commands are supported. If either path to the executable or the parameters contain spaces they <emphasis>must</emphasis> be wrapped in quotes.</para>

<para>Given the executable file <filename>C:\Program Files\Mozilla Firefox\firefox.exe</filename> the local html file <filename>C:\test file.html</filename> the correct line for the <guilabel>Executable</guilabel> would be: <filename>"C:\Program Files\Mozilla Firefox\firefox.exe" "C:\test file.html"</filename>.</para>

<para>The working folder defines where the process should be launched from. Given the working folder <filename>C:\folder</filename>, the command <filename>"C:\Program Files\Mozilla Firefox\firefox.exe" file.html</filename> would cause Firefox to search for the file <filename>C:\folder\file.html</filename>.</para>

<para>The working folder usually does not need to be set and can be left blank most of the time.</para>

<sect3 id="importing_executable_commands">
<title>Importing Programs</title>
<para>For even easier configuration Simon provides an import dialog which allows you to select programs directly from the KDE menu.</para>

<note><para>This option is not available on Microsoft Windows.</para></note>

<screenshot>
<screeninfo>Import Programs</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_program_import_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>The dialog will list all programs that have an entry in your KDE menu in their respective category.</para>

<para>Sub-Categories are not supported and are thus listed on the same level as top-level categories.</para>

<para>Just select the program you wish to start with Simon and press <guibutton>Ok</guibutton>. The correct values for the executable and the working folder as well as an appropriate command name and description will automatically be filled out for you.</para>

</sect3>

</sect2>

<sect2 id="place_commands">
<title>Place Commands</title>
<para>With place commands you can allow Simon to open any given URL. Because Simon just hands the address over to the platforms URL handler, special Protocols like <quote>remote:/</quote> (on &Linux;/&kde;) or even &kde;'s <quote>Web-Shortcuts</quote> are supported.</para>
<para>Instead of folders, files can also be set as the commands URL which will cause the file to be opened with the application which is associated with it when the command is invoked.</para>

<screenshot>
<screeninfo>Places</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>To associate a specific URL with the command you can manually enter it in the URL field (select <guibutton>Manual</guibutton> first) or import it with the import place wizard.</para>


<sect3 id="importing_place_commands">
<title>Importing Places</title>

<para>The import place dialog allows you to easily create the correct URL for the command.</para>

<para>To add a local folder, select <guibutton>Local Place</guibutton> and choose the folder or file with the file selector.

<screenshot>
<screeninfo>Import Places: Local</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place_import_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To add a remote URL (HTTP, FTP, &etc;) choose <guibutton>Remote URL</guibutton>.</para>

<screenshot>
<screeninfo>Import Places: Remote</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_place_import_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>Please note that for URLs with authentication information the password will be stored in clear text.</para>

</sect3>
</sect2>

<sect2 id="shortcut_commands">
<title>Shortcut Commands</title>
<para>Using shortcut commands the user can associate commands with key-combinations.</para>

<para>The command will simulate keyboard input to trigger shortcuts like <keycombo>&Ctrl;<keycap>C</keycap></keycombo> or <keycombo>&Alt;<keycap>F4</keycap></keycombo>.</para>
<para>The plugin can press, release or press and release the configured key combination.

<screenshot>
<screeninfo>Defining Shortcut Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_shortcut.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>To select the shortcut you wish to simulate just toggle the shortcut button and press the key combination on your keyboard.</para>
<para>Simon will capture the shortcut and associate it with the command.</para>
<para>Due to technical limitations there are several shortcuts on Microsoft Windows that cannot be captured by Simon (this includes &eg; <keycombo>&Ctrl;&Alt;<keycap>Del</keycap></keycombo> and <keycombo>&Alt;<keycap>F4</keycap></keycombo>). These special shortcuts can be selected from a list below the aforementioned shortcut button.</para>

<note><para>This selection box is not visible in the screenshot above as the list is only displayed in the Microsoft Windows version of Simon.</para></note>
</sect2>

<sect2 id="text_macro_commands">
<title>Text-Macro Commands</title>
<para>Using text-macro commands, the user can associate text with a command. When the command is invoked, the associated text will be <quote>written</quote> by simulating keystrokes.</para>

<para>
<screenshot>
<screeninfo>Defining Text-Macro Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_text-macro.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2 id="list_command">
<title>List Commands</title>
<para>The list command is designed to combine multiple commands (all types of commands are supported) into one list. The user can then select the n-th entry by saying the associated number (1-9).</para>

<para>This is very useful to limit the amount of training required and provides the possibility to keep the vocabulary to a minimum.</para>

<para>
<screenshot>
<screeninfo>Defining List Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>List commands are especially useful when using commands with difficult triggers or commands that can be grouped under a general theme. A typical example would be a command <quote>Startmenu</quote> to present a list of programs to launch. That way the specific executable commands can still retain very descriptive names (like <quote>OpenOffice.org Writer 3.1</quote>) without the user having to include these words in his vocabulary and consider them in the grammar just to trigger them.</para>

<para>Commands of different types can of course be mixed.</para>

<sect3 id="list_command_display">
<title>List Command Display</title>

<para>
When invoked, the command will display the list centered on the screen. The list will automatically expand to accompany its items.</para>
<screenshot>
<screeninfo>Defining List Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_2.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
<para>The user can invoke the commands contained in the list by simply saying their associated number (In this example: <quote>One</quote> to launch Mozilla Firefox).
</para>

<para>
While a list command is active (displayed), all input that is not directed at the list itself (other commands, &etc;) will be rejected. The process can be canceled by pressing the <guibutton>Cancel</guibutton> button or by saying <quote>Cancel</quote>.</para>

<para>
If there are more than 9 items Simon will add <quote>Next</quote> and <quote>Back</quote> options to the list (<quote>Zero</quote> will be associated with <quote>Back</quote> and <quote>Nine</quote> with <quote>Next</quote>).

<screenshot>
<screeninfo>List Command with many entries</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_list_3.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect3>

<sect3 id="list_command_configure">
<title>Configuring list elements</title>

<para>
By default the list command uses the following trigger words. To use list commands to their full potential, make sure that your language and acoustic model contains and allows for the following <quote>sentences</quote>:
<itemizedlist>
<listitem><para><quote>Zero</quote></para></listitem>
<listitem><para><quote>One</quote></para></listitem>
<listitem><para><quote>Two</quote></para></listitem>
<listitem><para><quote>Three</quote></para></listitem>
<listitem><para><quote>Four</quote></para></listitem>
<listitem><para><quote>Five</quote></para></listitem>
<listitem><para><quote>Six</quote></para></listitem>
<listitem><para><quote>Seven</quote></para></listitem>
<listitem><para><quote>Eight</quote></para></listitem>
<listitem><para><quote>Nine</quote></para></listitem>
<listitem><para><quote>Cancel</quote></para></listitem>
</itemizedlist>
</para>

<para>Of course you can also configure these words in your Simon configuration:
  <itemizedlist>
    <listitem>
      <para><guimenuitem>Commands</guimenuitem> > <guibutton>Manage plugins</guibutton> > <guimenuitem>General</guimenuitem> > <guimenuitem>Lists</guimenuitem> for the scenario wide list configuration.</para>
    </listitem>
    <listitem>
      <para><guimenu>Settings</guimenu> > <guimenuitem>Configure Simon...</guimenuitem> > <guimenuitem>Actions</guimenuitem> > <guimenuitem>Lists</guimenuitem> for the global configuration. When creating a new scenario, the scenario configuration will be initialized with a copy of this list configuration.</para>
    </listitem>
  </itemizedlist>
</para>

<para>
List commands are internally also used by other plugins like for example the <link linkend="desktopgrid">desktop grid</link>. The configuration of the triggers also affects their displayed lists.
</para>

</sect3>

</sect2>

<sect2 id="composite_commands">
<title>Composite Commands</title>
<para>Composite commands allow the user to group multiple commands into a sequence.</para>

<para>When invoked the commands will be executed in order. Delays between commands can be inserted.</para>

<para>Composite commands can also work as "transparent wrappers" by selecting <guibutton>Pass recognition result through to other commands</guibutton>. In that case, the recognition result will be treated as "unprocessed" even if the composite command was executed.</para>
<para>For example, suppose you have a command to turn on the light in one scenario. Additionally to turning on the light, you now want to add some kind of reporting to the activity by invoking a script through a <link linkend="executable_commands">program plugin</link>. You could then set up a reporting scenario that contains a transparent composite command with the same trigger as the command to turn on the light and make sure that this scenario is set before the original one in the scenario list. You can then activate and deactivate the reporting simply by loading and unloading this scenario.</para>

<para>
<screenshot>
<screeninfo>Defining Composite Commands</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_composite_1.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
Using the composite command the user can compose complex <quote>macros</quote>. The screenshot above - for example - does the following:
<itemizedlist>
<listitem><para>Start Kopete (Executable Command)</para></listitem>
<listitem><para>Wait 2000ms for Kopete do be started</para></listitem>
<listitem><para>Type <quote>Mathias</quote> (Text-Macro Command) which will select Mathias in my contact list</para></listitem>
<listitem><para>Press Enter (Shortcut Command)</para></listitem>
<listitem><para>Wait 1000ms for the chat window to appear</para></listitem>
<listitem><para>Write <quote>Hi!</quote> (Text-Macro Command); the text associated to this command contains a newline at the end so that the message will be send.</para></listitem>
<listitem><para>Press <keycombo>&Alt;<keycap>F4</keycap></keycombo> (Shortcut Command) to close the chat window</para></listitem>
<listitem><para>Press <keycombo>&Alt;<keycap>F4</keycap></keycombo> (Shortcut Command) to close the kopete main window</para></listitem>
</itemizedlist>
</para>

</sect2>

<sect2 id="desktopgrid">
<title>Desktop grid</title>
<para>The desktop grid allows the user to control his mouse with his voice.</para>

<para>
<screenshot>
<screeninfo>The Desktopgrid</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_desktopgrid.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
The desktop grid divides the screen into nine parts which are numbered from 1-9. Saying one of these numbers will again divide the selected field into 9 fields again numbered from 1-9, &etc; This is repeated 3 times. After the fourth time the desktop grid will be closed and Simon will click in the middle of the selected area.</para>

<para>The exact click action is configurable but defaults to asking the user. Therefore you will be presented with a list of possible click modes. When selecting Drag and Drop, the desktop grid will be displayed again to select the drop point.</para>

<para>
<screenshot>
<screeninfo>Desktopgrid: Click selection</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktop_grid_click_selection.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
While the desktop grid is active (displayed), all input that is not directed at the desktop grid itself (other commands, &etc;) will be rejected. Say <quote>Cancel</quote> at any time to abort the process.
</para>

<para>The desktop grid plugin registers a configuration screen right in the command configuration when it is loaded.</para>

<para>
<screenshot>
<screeninfo>Configuring the Desktopgrid</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktopgrid_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
The trigger that invokes the desktop grid is of course completely configurable. Moreover the user can use <quote>real</quote> or <quote>fake</quote> transparency. If your graphical environment allows for compositing effects (<quote>desktop effects</quote>) then you can safely use <quote>real</quote> transparency which will make the desktop grid transparent. If your platform does not support compositing Simon will simulate transparency by taking a screenshot of the screen before displaying the desktop grid and display that picture behind the desktop grid.
</para>

<para>If the desktop grid is configured to use real transparency and the system does not support compositing it will display a solid gray background.</para>

<para>
However, nearly all up-to-date systems will support compositing (real transparency).
</para>
<para>This includes:
<itemizedlist>
<listitem><para>Microsoft Windows 2000 or higher (XP, Vista, 7)</para></listitem>
<listitem><para>GNU/Linux using a composite manager like Compiz, KWin4, xcompmgr, &etc;</para></listitem>
</itemizedlist>
</para>


<para>
By default the desktop grid uses numbers to select the individual fields. To use the desktop grid, make sure that your language and acoustic model contains and allows for the following <quote>sentences</quote>:
<itemizedlist>
<listitem><para><quote>One</quote></para></listitem>
<listitem><para><quote>Two</quote></para></listitem>
<listitem><para><quote>Three</quote></para></listitem>
<listitem><para><quote>Four</quote></para></listitem>
<listitem><para><quote>Five</quote></para></listitem>
<listitem><para><quote>Six</quote></para></listitem>
<listitem><para><quote>Seven</quote></para></listitem>
<listitem><para><quote>Eight</quote></para></listitem>
<listitem><para><quote>Nine</quote></para></listitem>
<listitem><para><quote>Cancel</quote></para></listitem>
</itemizedlist>
</para>

<para>To configure these triggers, just configure the commands associated with the plugin.</para>

<para>
<screenshot>
<screeninfo>Desktopgrid: Configuring list elements</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="desktop_grid_configure_list_elements.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="input_number">
<title>Input Number</title>
<para>Using the input-number plugin the user can input large numbers easily.</para>


<para>Using the Dictation or the Text-Macro plugin one could associate the numbers with their digits and use that as input method. However, to input larger numbers there are two ways that both have significant disadvantages:

<itemizedlist>
<listitem><para><emphasis>Adding the words <quote>eleven</quote>, <quote>twelve</quote>, &etc;</emphasis></para>
<para>While this seems like the most elegant solution as it would enable the user to say <quote>fivehundredseventytwo</quote> we can easily see that it would be quite a problem to add all these words - let alone train them. What about <quote>twothousandninehundredtwo</quote>? Where to stop?</para></listitem>
<listitem><para><emphasis>Spell out the number using the individual digits</emphasis></para>
<para>While this is not as elegant as stating the complete number it is much more practical.</para>
<para>However, many applications (like the great mouseless browsing firefox addon) rely on the user to input large numbers without too much time passing between the individual keystrokes (mouseless browsing for example will wait exactly 500ms per default before it considers the input of the number complete). So if you want to enter 52 you would first say <quote>Five (pause) Two</quote>. Because of the needed pause, the application (like the mouseless browsing plugin) would consider the input of <quote>Five</quote> complete.</para></listitem>
</itemizedlist>
</para>

<para>
The input number plugin - when triggered - presents a calculator-like interface for inputting a number. The input can be corrected by saying <quote>Back</quote>. It features a decimal point accessible by saying <quote>Comma</quote>. When saying <quote>Ok</quote> the number will be typed out. As all the voice-input and the correction is handled by the plugin itself the application that finally receive the input will only get couple of milliseconds between the individual digits.

<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="command_input_number.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>
While the input number plugin is active (the user currently inputs a number), all input that is not directed at the input number plugin (other commands, &etc;) will be rejected. Say <quote>Cancel</quote> at any time to abort the process.
</para>

<para>As there can no command instances be created of this plugin it is not listed in the <guilabel>New Command</guilabel> dialog. However, the input number plugin registers a configuration screen right in the command configuration when it is loaded.</para>

<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="input_number_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>
The trigger defines what word or phrase that will trigger the display of the interface.
</para>


<para>
By default the input number plugin uses numbers to select the individual digits and a couple of control words. To use the input number plugin, make sure that your language and acoustic model contains and allows for the following <quote>sentences</quote>:
<itemizedlist>
<listitem><para><quote>Zero</quote></para></listitem>
<listitem><para><quote>One</quote></para></listitem>
<listitem><para><quote>Two</quote></para></listitem>
<listitem><para><quote>Three</quote></para></listitem>
<listitem><para><quote>Four</quote></para></listitem>
<listitem><para><quote>Five</quote></para></listitem>
<listitem><para><quote>Six</quote></para></listitem>
<listitem><para><quote>Seven</quote></para></listitem>
<listitem><para><quote>Eight</quote></para></listitem>
<listitem><para><quote>Nine</quote></para></listitem>
<listitem><para><quote>Back</quote></para></listitem>
<listitem><para><quote>Comma</quote></para></listitem>
<listitem><para><quote>Ok</quote></para></listitem>
<listitem><para><quote>Cancel</quote></para></listitem>
</itemizedlist>
</para>

<para>To configure these triggers, just configure the commands associated with the plugin.</para>

<para>
<screenshot>
<screeninfo>Input Number Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_number_input_elements.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="dictation_command_plugin">
<title>Dictation</title>
<para>The dictation plugin writes the recognition result it gets using simulated keystrokes.</para>

<para>Assuming you didn't define a trigger for the dictation plugin it will accept all recognition results and just write them out. The written input will be considered as <quote>processed input</quote> and thus not be relayed to other plugins. This means that if you loaded the dictation plugin and defined no trigger for it, all plugins <emphasis>below it</emphasis> in the <guilabel>Selected Plug-Ins</guilabel> list in the command configuration will never receive any input.</para>

<para>As there can no command instances be created of this plugin it is not listed in the <guilabel>New Command</guilabel> dialog.</para>

<para>The dictation plugin can be configured to append texts after recognition results to for example add a space after each recognized word.</para>

<para>
<screenshot>
<screeninfo>Configure dictation</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="configure_dictation.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>
</sect2>

<sect2 id="ai_command_plugin">
<title>Artificial Intelligence</title>
<para>The Artificial Intelligence is a just-for-fun plugin that emulates a human conversation.</para>

<para>Using the <link linkend="text-to-speech_config">text to speech system</link>, the computer can <quote>talk</quote> with the user.</para>

<para>The plugin uses AIMLs for the actual <quote>intelligence</quote>. Most AIML sets should be supported. The popular <ulink url="http://www.pandorabots.com/pandora/talk?botid=f5d922d97e345aa1">A. L. I. C. E. bot</ulink> and a German version work and are shipped with the plugin.</para>

<screenshot>
<screeninfo>AI Plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="ai_configure.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>

<para>
The plugin registers a configuration screen in the command configuration menu where you can choose which AIML set to load.
</para>

<para>Simon will look for AIML sets in the following folder:
<itemizedlist>
<listitem><para>GNU/Linux: <filename>`kde4-config --prefix`/share/apps/ai/aimls/</filename></para></listitem>
<listitem><para>Microsoft Windows: <filename>[installation folder (C:\Program Files\simon 0.2\ by default)]\share\apps\ai\aimls\</filename></para></listitem>
</itemizedlist>
To add a new set just create a new folder with a descriptive name and copy the .aiml files into it.
</para>

<para>
To adjust your bots personality have a look at the bot.xml and vars.xml files in the following folder:
<itemizedlist>
<listitem><para>GNU/Linux: <filename>`kde4-config --prefix`/share/apps/ai/util/</filename></para></listitem>
<listitem><para>Microsoft Windows: <filename>[installation folder (C:\Program Files\simon 0.2\ by default)]\share\apps\ai\util\</filename></para></listitem>
</itemizedlist>
</para>

<para>As there can no command instances be created of this plugin it is not listed in the <guilabel>New Command</guilabel> dialog.</para>

<para>It is recommended to not use any trigger for this plugin to provide a more natural <quote>feel</quote> for the conversation.</para>
</sect2>

<sect2 id="calculator_command_plugin">
<title>Calculator</title>
<para>The calculator plugin is a simple, voice controlled calculator.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>The calculator extends the <link linkend="input_number">Input Number</link> plugin by providing additional features.</para>

<para>When loading the plugin, a configuration screen is added to the plugin configuration.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Configuration</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_configuration.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>There you can also configure the control mode of the calculator. Setting the mode to something else than <guimenuitem>Full calculator</guimenuitem> will hide options from the displayed widget.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Minimal</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_minimal.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

<para>However, the hidden controls will, in contrast to simply removing all associated command from the functions, still react to the configured voice commands.</para>

<para>When selecting <guibutton>Ok</guibutton>, the calculator will by default ask you what to do with the generated result. You can for example output the calculation, the result, both, &etc; Besides always selecting this from the displayed list after selecting the <guibutton>Ok</guibutton> button, this can also be set in the configuration options.</para>

<para>
<screenshot>
<screeninfo>Calculator plugin: Output mode selection</screeninfo>
  <mediaobject>
    <imageobject>
      <imagedata fileref="calculator_output_mode_selection.png" format="PNG"/>
    </imageobject>
  </mediaobject>
</screenshot>
</para>

</sect2>

<sect2 id="filter_command_plugin">
  <title>Filter</title>
  <para>Using the filter plugin, you can intercept recognition results from being passed on to further command plugins. Using this plugin you can for example disable the recognition by voice.</para>
  <para>
    The filter command plugin registers a configuration screen in the command configuration where you can change what results should be filtered.
  </para>

  <para>
  <screenshot>
  <screeninfo>Filter plugin: Configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="filter_configuration.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The pattern is a regular expression that will be evaluated each time a recognition results receives the plugin for processing.</para>

  <para>The plugin also registers voice interface commands for activating and deactivating the filter.</para>

  <para>In total, the filter therefore has three states:
    <itemizedlist>
      <listitem>
        <para>Inactive</para>
        <para>The default state. All recognition results will be passed through.</para>
      </listitem>
      <listitem>
        <para>Half-active (if <guibutton>Two stage activation</guibutton> is selected)</para>
        <para><itemizedlist>
	<listitem><para>If the next command is the "Deactivate filter" command, the filter will enter the "Inactive" state.</para></listitem>
	<listitem><para>If, however, the next result is something else and <guibutton>Relay results in stage one of two stage activation</guibutton> is selected, this result will be passed on to other plugins. The filter will reset to "Active" afterwards.</para></listitem>
	</itemizedlist></para>
      </listitem>
      <listitem>
        <para>Active</para>
        <para>When activated, the filter will <quote>eat</quote> all results that match the configured pattern. By default this means every result that Simon recognizes will be accepted by the filter and therefore not relayed to any of the plugins following the filter plugin.</para>
	<para>If <guibutton>Two stage activation</guibutton> is enabled and the filter plugin receives the command to directly enter the "Inactive" state, this command is ignored. In other ways: If two stage activation is enabled, the filter can only be disabled by going through the intermediate stage.</para>
      </listitem>
    </itemizedlist>
  </para>
</sect2>

<sect2 id="pronunciation_training_command_plugin">
  <title>Pronunciation Training</title>

  <para>The pronunciation training, when combined with a good <link linkend="base_model">static base model</link>, can be a powerful tool to improve your pronunciation of a new language.</para>

  <para>
  <screenshot>
  <screeninfo>Pronunciation training</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="pronunciation_training.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Essentially, the plugin will prompt you to say specific words. The recognition will then recognize your pronunciation of the word and compare it to your speech model which should be a base model of native speakers for this to work correctly. Then Simon will display the recognition rate (how similar your version was to the stored base model).</para>

  <para>The closer to the native speaker, the higher the score.</para>

  <para>The plugin adds an entry to your <guimenu>Commands</guimenu> menu to launch the pronunciation training dialog.</para>

  <para>The training itself consists of multiple pages. Each page contains one word fetched from your active vocabulary. They are identified by a category which needs to be selected in the command configuration before starting the training.</para>

  <para>
  <screenshot>
  <screeninfo>Pronunciation training: Configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="pronunciation_training_configuration.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect2>

<sect2 id="keyboard_command_plugin">
  <title>Keyboard</title>

  <para>The keyboard plugin displays a virtual, voice controlled keyboard.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_1.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The keyboard consists of multiple tabs, each possibly containing many keys. The entirety of tabs and keys are collected in <quote>sets</quote>.</para>

  <para>You can select sets in the configuration but also create new ones from scratch in the keyboard command configuration.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_2.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Keys are usually mapped to single characters but can also hold long texts and even shortcuts. Because of this, keyboard sets can contain special keys like a <quote>select all</quote> key or a <quote>Password</quote> key (typing your password).</para>

  <para>Next to the tabs that hold the keys of your set, the keyboard may also show special keys like &Ctrl;, &Shift;, &etc; Those keys are provided as voice interface commands and are displayed regardless of what tab of the set is currently active.</para>
  <para>As with all voice triggers, removing the associated command, hides the buttons as well.</para>

  <para>Moreover, the keyboard provides a numpad that can be shown by selecting the appropriate option in the keyboard configuration.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Keypad</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_3.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Next to the number keys and the delete key for the number input field (<guibutton>Number backspace</guibutton>), the numpad provides two options on what to do with the entered number.</para>
  <para>When selecting <guibutton>Write number</guibutton>, the entered number will be written out using simulated key presses. Selecting <guibutton>Select number</guibutton> tries to find a key or tab in the currently active set that has this number as a trigger. This way you can control a complete keyboard just using numbers.</para>

  <para>
  <screenshot>
  <screeninfo>Keyboard: Number based</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="keyboard_4.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The keys on the num pad are configurable voice interface commands.</para>
</sect2>

<sect2 id="dialog_command_plugin">
  <title>Dialog</title>
  
  <para>The dialog plugin enables users to engage in a scripted dialog with &kmyapplication;.</para>

  <sect3 id="dialog_design">
  <title>Dialog design</title>

  <para>&kmyapplication; treats dialogs as a succession of different states. Each state can have a text and several associated options.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog design</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_design.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>


  <para>Dialogs can have more than one text variants - one of which will be randomly picked when the dialog is displayed. This can help to make dialogs feel more natural by providing several, alternative formulations.</para>

  <para>The texts can use <link linkend="dialog_bound_values">bound values</link> and <link linkend="dialog_template_options">template options</link>.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog design: Options</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_design_options.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Dialog options capsule the logic of the conversation. They are the active components of the dialog.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog design: Adding an option</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_design_options_new.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>Similar to commands, dialog options have a name (trigger) that, when recognized while the dialog is active and in the option's parent state, will cause this option to activate. Alternatively, options can also be configured to trigger automatically after a set time period. This time is relative to when the state is entered.</para>

  <para>Dialog options, when shown through the <link linkend="dialog_output">graphical output module</link> can show an arbitrary text (that will most likely be equivalent to the trigger but doesn't have to be) and, optionally, an icon. If the text-to-speech output module is used, the text (not the trigger) will be read aloud unless this is disabled by selecting the <guibutton>Silent</guibutton> option.</para>

  <para>Every state can also optionally have an avatar that will be displayed when using the <link linkend="dialog_output">graphical output module</link>.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog design: Avatar</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_design_avatar.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  </sect3>

  <sect3 id="dialog_bound_values">
  <title>Dialog: Bound values</title>
  <para>The text of dialog states can contain variables - so called "bound values" - that will be filled in during runtime.</para>

  <para>For example, the dialog text "This is a $variable$" would replace "$variable$" with the result of a bound value called "variable".</para>

  <para>
  <screenshot>
  <screeninfo>Bound values</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_bound_values.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>There are four types of bound values:
  <itemizedlist>
  <listitem>
    <para>Static</para>
    <para>
    <screenshot>
    <screeninfo>Bound values: Static</screeninfo>
      <mediaobject>
        <imageobject>
          <imagedata fileref="dialog_bound_value_add_static.png" format="PNG"/>
        </imageobject>
      </mediaobject>
    </screenshot>
    </para>
    <para>Static bound values will always be resolved to the same text. They are useful to provide configuration options to be filled in to personalize the dialog (&eg;, the name of the user).</para>
  </listitem>
  <listitem>
    <para>QtScript</para>
    <para>
    <screenshot>
    <screeninfo>Bound values: QtScript</screeninfo>
      <mediaobject>
        <imageobject>
          <imagedata fileref="dialog_bound_value_add_qtscript.png" format="PNG"/>
        </imageobject>
      </mediaobject>
    </screenshot>
    </para>
    <para>QtScript bound values resolve to the result of the entered <ulink url="http://doc.qt.digia.com/qt/ecmascript.html">QtScript</ulink> code.</para>
  </listitem>
  <listitem>
    <para>Command arguments</para>
    <para>
    <screenshot>
    <screeninfo>Bound values: Command arguments</screeninfo>
      <mediaobject>
        <imageobject>
          <imagedata fileref="dialog_bound_value_add_argument.png" format="PNG"/>
        </imageobject>
      </mediaobject>
    </screenshot>
    </para>
    <para>If the dialog trigger command (the &kmyapplication; command that initiates the dialog) uses <link linkend="command_arguments">placeholders</link>, they can be accessed through command argument bound values. The <guilabel>Argument</guilabel> number refers to the index of the placeholder you want to access.</para>
    <para>For example, if your dialog is started with the command "Call %1", and "name" is a command argument bound value, then launching the dialog by recognizing "Call Peter", will turn the dialog text "Are you sure you want to call $name$?" into "Are you sure you want to call Peter?".</para>
  </listitem>
  <listitem>
    <para>Plasma data engine</para>
    <para>
    <screenshot>
    <screeninfo>Bound values: Plasma data engine</screeninfo>
      <mediaobject>
        <imageobject>
          <imagedata fileref="dialog_bound_value_add_plasma.png" format="PNG"/>
        </imageobject>
      </mediaobject>
    </screenshot>
    </para>
    <para>This type of bound value can readily access a wide array of high-level information through plasma data engines.</para>
  </listitem>
  </itemizedlist>
  </para>
  </sect3>

  <sect3 id="dialog_template_options">
  <title>Template options</title>
  <para>Dialog texts can further be parametrized through template options.</para>

  <para>
  <screenshot>
  <screeninfo>Template options</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_template_options.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
  <para>These boolean values choose between different or optional text snippets.</para>

  <para>For example, the template option "formal" above, would change the dialog text "Would you please {{{formal}}be quiet{{elseformal}}shut up{{endformal}}" to "Would you please be quit" or "Would you please shut up" depending on if the template option is set to true or false. The else-path can be omitted if it is not required (&eg; "Would you {{formal}}please {{endformal}}be quiet").</para>
  </sect3>

  <sect3 id="dialog_avatars">
  <title>Avatars</title>
  <para>Every state can potentially show a different avatar.</para>

  <para>These images can range from the picture of a (simulated) speaker to an image of something topically appropriate.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog: Avatars</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_avatars.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
  <para>To use an avatar, first add it here and later define where to use it in the <link linkend="dialog_design">dialog design section</link>.</para>
  </sect3>

  <sect3 id="dialog_output">
  <title>Output</title>
  <para>Dialogs can be displayed graphically, use text-to-speech or combine both approaches.</para>

  <para>
  <screenshot>
  <screeninfo>Dialog: Output</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dialog_output.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The <guilabel>Separator to options</guilabel> will be spoken between the dialog text and the current state's options (if there are any). If there are no options to this state or all are configured to be silent, this will not be said. The option to listen to the whole announcement again is triggered when saying one of the configured <guilabel>Repeat on trigger</guilabel>. Additionally, the text-to-speech output can optionally be configured to repeat the listing of the available options (including the configured separator) when the user says a command that does not match any of the available dialog options.</para>
  </sect3>
</sect2>

<sect2 id="akonadi_command_plugin">
  <title>Akonadi</title>
  
  <para>The Akonadi plugin allows &kmyapplication; to plug into KDE's PIM infrastructure.</para>

  <para>
  <screenshot>
  <screeninfo>Akonadi command configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="akonadi_command_configuration.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>The plugin fulfills two major purposes:
    <itemizedlist>
    <listitem>
      <para>Execute Simon commands at scheduled times</para>
      <para>The Akonadi plugin can monitor a specific collection (calendar) and react on entries whose summary start with a specific prefix. Per default, this prefix is "[simon-command]", meaning that events of the form "[simon-command] &lt;plugin name&gt;//&lt;command name&gt;" will trigger the appropriate &kmyapplication; command at the "start time" of the event.</para>

      <para>The name of the plugins and commands are equivalent to the ones shown in the <link linkend="commands">command dialog</link> and do not necessarily need to reference commands in the same scenario as the Akonadi plugin instance.</para>
    </listitem>
    <listitem>
      <para>Show reminders for events in the given calendar</para>
      <para>If configured to do so, the Akonadi plugin can show reminders for calendar events with a set alarm flag. These reminders will be shown through the &kmyapplication; <link linkend="dialog_command_plugin">dialog engine</link>.</para>
   </listitem>
    </itemizedlist>
  </para>
</sect2>

<sect2 id="dbus_command_plugin">
  <title>D-Bus</title>
  
  <para>With the D-Bus command plugin, Simon can call exported methods in 3rd party applications directly.</para>

  <para>The screenshot below, for example, calls the "Pause" method of the MPRIS interface of the Tomahawk music playing software.</para>

  <para>
  <screenshot>
  <screeninfo>D-Bus command</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="dbus_command.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect2>

<sect2 id="json_command_plugin">
  <title>JSON</title>
  
  <para>Similar to the <link linkend="dbus_command_plugin">D-Bus command plugin</link>, the JSON plugin also allows to contact 3rd party applications to directly invoke functionality (instead of simulating user activity).</para>

  <para>
  <screenshot>
  <screeninfo>JSON command</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="json_command.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
</sect2>

<sect2 id="vrpn_command_plugin">
  <title>VRPN</title>

  <para>With the VRPN command plugin, Simon can act as a VRPN server and export voice controlled buttons.</para>


  <para>
  <screenshot>
  <screeninfo>VRPN plugin configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="vrpn_command_config.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>
  <para>The plugin configuration allows you to set the port the server should operate on and to define an arbitrary list of buttons. Each of these button objects will have exactly one "button" (in VRPN, a button may theoretically have more than one clickable item).</para>

  <para>After setting up the buttons, you can now configure Simon commands to act on them. You can set the commands to either Press &amp; Release (consecutively), Press, Release or Toggle the button they manipulate.</para>

  <para>
  <screenshot>
  <screeninfo>VRPN command configuration</screeninfo>
    <mediaobject>
      <imageobject>
        <imagedata fileref="vrpn_command.png" format="PNG"/>
      </imageobject>
    </mediaobject>
  </screenshot>
  </para>

  <para>For example, the command shown in the screenshot above would press and release ("click") the VRPN button at index 0 of the button device accessible as "ButtonB@localhost".</para>
</sect2>

<!--
     Not included in this version of Simon
<sect2 id="atspi_command_plugin">
  <title>AT-SPI</title>
  
</sect2>
-->

</sect1>



</chapter>



<chapter id="faq">
<title>Questions and Answers</title>

<para>
In an effort to keep this section always up-to-date it is available at our <ulink url="http://userbase.kde.org/Special:myLanguage/Simon/Troubleshooting_Guide">online wiki</ulink>.
</para>

</chapter>

<chapter id="credits">

<title>Credits and License</title>

<para>
&kmyapplication;
</para>
<para>
Program copyright 2006-2009 Peter Grasch <email>peter.grasch@bedahr.org</email>, Phillip Goriup, Tschernegg Susanne, Bettina Sturmann, Martin Gigerl
</para>

<para>
Documentation Copyright &copy; 2009 Peter Grasch <email>peter.grasch@bedahr.org</email>
</para>

<!-- TRANS:CREDIT_FOR_TRANSLATORS -->

&underFDL;               <!-- FDL: do not remove -->

<!-- Determine which license your application is licensed under,
     and delete all the remaining licenses below:

     (NOTE:  All documentation are licensed under the FDL,
     regardless of what license the application uses) -->

&underGPL;        	 <!-- GPL License -->

</chapter>

<appendix id="installation">
<title>Installation</title>
<para>Please see our <ulink url="http://userbase.kde.org/Special:myLanguage/Simon/Installation">wiki</ulink> for install instructions.</para>

</appendix>

&documentation.index;
</book>

<!--
Local Variables:
mode: xml
sgml-minimize-attributes:nil
sgml-general-insert-case:lower
sgml-indent-step:0
sgml-indent-data:nil
End:

vim:tabstop=2:shiftwidth=2:expandtab
kate: space-indent on; indent-width 2; tab-width 2; indent-mode none;
-->
